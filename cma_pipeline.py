"""
CMA Pipeline - å®Œæ•´ç‰ˆ
åŒæ—¶æ”¯æŒ:
1. æœ¬åœ°æ¨¡å‹ / OpenAI API åˆ‡æ¢
2. æ•°æ®åŠ è½½åŠŸèƒ½
3. çœŸå®å›¾å¯¹æ¯”è¯„ä¼°
"""

import json
import numpy as np
import os
from typing import Dict, List, Optional

from llm_hypothesis import LLMHypothesisGenerator
from model_fitting import ModelFittingEngine
from post_processing import PostProcessor
from data_loader import DataLoader, CausalDataset, DOMAIN_CONTEXTS
from skeleton_builder import SkeletonBuilder, _skeleton_to_graph_format
from transformers import AutoTokenizer
from metrics import _compute_metrics
from search_strategies import HillClimbingStrategy, MCTSStrategy
from baseline_reference import load_baseline_reference_from_predict
from intervention_utils import InterventionTester, EvidencePolicyVerifier

os.environ['VLLM_ATTENTION_BACKEND'] = 'FLASH_ATTN'

class CMAPipeline:
    """CMAå®Œæ•´æµç¨‹ç®¡ç†å™¨ - æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œæ•°æ®åŠ è½½"""
    
    def __init__(
        self,
        # æ•°æ®å‚æ•° - ä¸‰é€‰ä¸€
        domain_name: str = None,
        variable_list: List[str] = None,
        data: np.ndarray = None,
        dataset: CausalDataset = None,  # æ–°å¢: ç›´æ¥ä¼ å…¥æ•°æ®é›†
        domain_context: str = "",
        use_observational_only: bool = True,  # æ–°å¢: æ˜¯å¦åªç”¨è§‚æµ‹æ•°æ®
        
        # éª¨æ¶æ„å»ºå‚æ•°
        use_skeleton: bool = False,  # æ˜¯å¦ä½¿ç”¨MMHCéª¨æ¶
        skeleton_alpha: float = 0.05,  # éª¨æ¶æ„å»ºçš„æ˜¾è‘—æ€§æ°´å¹³
        skeleton_max_cond_size: int = 3,  # éª¨æ¶æ„å»ºçš„æœ€å¤§æ¡ä»¶é›†å¤§å°
        
        # åŸºçº¿å‚è€ƒå‚æ•°
        use_baseline_reference: bool = False,  # æ˜¯å¦ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•çš„å‚è€ƒä¿¡æ¯
        baseline_predict_dir: str = "predict",  # predict ç›®å½•è·¯å¾„
        baseline_methods: List[str] = None,  # è¦åŠ è½½çš„åŸºçº¿æ–¹æ³• ['corr', 'invcov', 'notears']
        baseline_top_k: int = 10,  # æ¯ä¸ªæ–¹æ³•æ˜¾ç¤ºtop-kä¸ªå…³ç³»
        baseline_threshold: float = 0.5,  # åŸºçº¿æ–¹æ³•çš„é˜ˆå€¼ç™¾åˆ†ä½
        choose_best: bool = False,  # æ˜¯å¦åœ¨åˆå§‹é˜¶æ®µæ¯”è¾ƒåŸºçº¿å’ŒLLM
        
        # å¹²é¢„æµ‹è¯•å‚æ•°
        use_intervention_test: bool = False,  # æ˜¯å¦å…è®¸LLMä¸»åŠ¨å‘èµ·å¹²é¢„å®éªŒ
        num_intervention_experiments: int = 3, # æ¯è½®å…è®¸çš„æœ€å¤§å®éªŒæ•°
        
        # ç»å…¸ç®—æ³•å¢å¼ºå‚æ•°
        use_notears_refinement: bool = False,  # æ˜¯å¦ä½¿ç”¨NOTEARSä¼˜åŒ–
        notears_use_mlp: bool = False,  # NOTEARSæ˜¯å¦ä½¿ç”¨MLPä½œä¸ºscoreï¼ˆæ¨èï¼‰
        notears_alpha: float = 0.001,  # NOTEARS L2æ­£åˆ™åŒ–ï¼ˆä»…Ridgeç‰ˆæœ¬ï¼‰
        notears_threshold: float = 0.15,  # NOTEARSè¾¹æƒé‡é˜ˆå€¼
        notears_poly_degree: int = 2,  # å¤šé¡¹å¼é˜¶æ•°ï¼ˆä»…Ridgeç‰ˆæœ¬ï¼‰
        notears_start_iter: int = 0,  # ä»ç¬¬å‡ è½®è¿­ä»£å¼€å§‹ä½¿ç”¨NOTEARS
        use_greedy_refinement: bool = False,  # æ˜¯å¦ä½¿ç”¨è´ªå¿ƒä¼˜åŒ–ï¼ˆæ¨èï¼‰
        greedy_max_modifications: int = 10,  # è´ªå¿ƒä¼˜åŒ–çš„æœ€å¤§ä¿®æ”¹æ¬¡æ•°
        greedy_min_improvement: float = 0.01,  # è´ªå¿ƒä¼˜åŒ–çš„æœ€å°LLæ”¹è¿›é˜ˆå€¼
        greedy_eval_epochs: int = 15,  # è´ªå¿ƒè¯„ä¼°æ—¶çš„è®­ç»ƒè½®æ•°ï¼ˆé™ä½ä»¥åŠ é€Ÿï¼‰
        greedy_max_candidates: int = 30,  # æ¯ç§æ“ä½œæœ€å¤šæµ‹è¯•çš„å€™é€‰æ•°ï¼ˆåŠ é€Ÿï¼‰
        greedy_start_iter: int = 0,  # ä»ç¬¬å‡ è½®è¿­ä»£å¼€å§‹ä½¿ç”¨è´ªå¿ƒä¼˜åŒ–
        
        # MCTSå‚æ•°
        mcts_simulations: int = 50,  # MCTSæ¯æ¬¡è¿­ä»£çš„æ¨¡æ‹Ÿæ¬¡æ•°
        mcts_exploration_weight: float = 1.414,  # MCTS UCB1æ¢ç´¢æƒé‡
        mcts_max_depth: int = 5,  # MCTSæœ€å¤§æœç´¢æ·±åº¦
        
        # è¾“å‡ºå‚æ•°
        output_dir: str = "./cma_output",
        device: str = 'cpu',
        
        # LLMé…ç½®
        llm_type: str = "openai",  # "openai" æˆ– "local"
        llm_model_path: str = None,  # æœ¬åœ°æ¨¡å‹è·¯å¾„
        openai_base_url: str = None,  # OpenAI API URL
        openai_api_key: str = None,   # OpenAI API key
        
        # é¢„åŠ è½½çš„æ¨¡å‹ï¼ˆç”¨äºæ‰¹é‡å®éªŒå¤ç”¨ï¼‰
        shared_tokenizer = None,
        shared_model = None
    ):
        """
        åˆå§‹åŒ–CMAæµç¨‹
        
        Args:
            # æ–¹å¼1: æ‰‹åŠ¨æŒ‡å®šæ•°æ®
            domain_name: é¢†åŸŸåç§°
            variable_list: å˜é‡åˆ—è¡¨
            data: æ•°æ® [n_samples, n_variables]
            domain_context: é¢†åŸŸèƒŒæ™¯çŸ¥è¯†
            
            # æ–¹å¼2: ä¼ å…¥CausalDatasetå¯¹è±¡
            dataset: CausalDatasetå¯¹è±¡(åŒ…å«æ•°æ®ã€çœŸå®å›¾ã€å˜é‡åç­‰)
            use_observational_only: æ˜¯å¦åªä½¿ç”¨è§‚æµ‹æ•°æ®(æ’é™¤å¹²é¢„æ ·æœ¬)
            
            # éª¨æ¶æ„å»ºé…ç½®
            use_skeleton: æ˜¯å¦ä½¿ç”¨MMHCç®—æ³•æ„å»ºç»Ÿè®¡éª¨æ¶ä½œä¸ºçº¦æŸ
            skeleton_alpha: ç‹¬ç«‹æ€§æ£€éªŒçš„æ˜¾è‘—æ€§æ°´å¹³
            skeleton_max_cond_size: æ¡ä»¶é›†çš„æœ€å¤§å¤§å°
            
            # è¾“å‡ºé…ç½®
            output_dir: è¾“å‡ºç›®å½•
            device: æ¨¡å‹æ‹Ÿåˆè®¾å¤‡
            
            # LLMé…ç½®
            llm_type: "openai" æˆ– "local"
            llm_model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„(llm_type="local"æ—¶å¿…éœ€)
            openai_base_url: OpenAI API base URL
            openai_api_key: OpenAI API key
        """
        
        # ===== å¤„ç†æ•°æ®è¾“å…¥(ä¸¤ç§æ–¹å¼) =====
        assert dataset is not None, "dataset is required"
        self.dataset = dataset
        self.domain_name = dataset.domain_name
        self.variable_list = dataset.variable_names
        self.variable_type = dataset.variable_type  # å­˜å‚¨å˜é‡ç±»å‹ (continuous/discrete)
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œå¹¶ä¿å­˜å¹²é¢„ä¿¡æ¯ä»¥ä¾¿åœ¨æ¨¡å‹æ‹Ÿåˆæ—¶ç²¾ç¡®å¤„ç†
        # ä¸å†ä½¿ç”¨ç®€å•çš„"å…¨æœ‰æˆ–å…¨æ— "ç­–ç•¥ï¼Œè€Œæ˜¯åœ¨æ‹Ÿåˆæ—¶é’ˆå¯¹æ¯ä¸ªå˜é‡ä½¿ç”¨æœªè¢«å¹²é¢„çš„æ ·æœ¬
        self.data = dataset.data
        self.interventions = dataset.interventions  # ä¿å­˜å¹²é¢„ä¿¡æ¯
        
        if dataset.interventions is not None:
            n_intervened = (dataset.interventions.sum(axis=1) > 0).sum()
            n_observational = len(dataset.data) - n_intervened
            print(f"[Info] Using all data with intervention-aware fitting:")
            print(f"  - Total samples: {len(dataset.data)}")
            print(f"  - Observational samples: {n_observational}")
            print(f"  - Samples with interventions: {n_intervened}")
            print(f"  - During fitting, each variable will only use samples where it was NOT intervened")
        else:
            print(f"[Info] Using data: {self.data.shape} (no interventions)")
        
        # ä½¿ç”¨é¢„å®šä¹‰çš„é¢†åŸŸèƒŒæ™¯
        if not domain_context:
            self.domain_context = DOMAIN_CONTEXTS.get(dataset.domain_name, "")
        else:
            self.domain_context = domain_context
        
        self.output_dir = output_dir
        self.device = device
        self.llm_type = llm_type
        self.use_skeleton = use_skeleton
        self.skeleton_constraints = None
        
        # ===== åŸºçº¿å‚è€ƒé…ç½® =====
        self.use_baseline_reference = use_baseline_reference
        self.baseline_predict_dir = baseline_predict_dir
        self.baseline_methods = baseline_methods or ['corr', 'invcov']
        self.baseline_top_k = baseline_top_k
        self.baseline_threshold = baseline_threshold
        self.baseline_reference_text = None
        self.baseline_structured_graphs = {}  # åˆå§‹ä¸ºç©ºå­—å…¸
        self.choose_best = choose_best
        
        # ===== å¹²é¢„æµ‹è¯•é…ç½® =====
        self.use_intervention_test = use_intervention_test
        self.num_intervention_experiments = num_intervention_experiments
        self.accumulated_evidence = []
        self.intervention_tester = None
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        if self.dataset:
            self._save_dataset_info()
        
        # ===== æ„å»ºç»Ÿè®¡éª¨æ¶(å¦‚æœå¯ç”¨) =====
        if use_skeleton:
            print("\n" + "="*70)
            print("BUILDING STATISTICAL SKELETON")
            print("="*70)
            
            # è·å–å˜é‡ç±»å‹
            variable_type = self.dataset.variable_type if self.dataset else "continuous"
            
            skeleton_builder = SkeletonBuilder(
                alpha=skeleton_alpha,
                max_cond_size=skeleton_max_cond_size,
                variable_type=variable_type
            )
            
            skeleton, pc_sets = skeleton_builder.build_skeleton(
                data=self.data,
                variable_names=self.variable_list,
                verbose=True
            )
            
            self.skeleton_constraints = skeleton_builder.skeleton_to_constraint(
                skeleton, self.variable_list
            )
            skeleton_graph = _skeleton_to_graph_format(skeleton, self.variable_list)
            skeleton_metrics = _compute_metrics(self, skeleton_graph)
            
            # æ‰“å°éª¨æ¶è´¨é‡
            print("\n" + "-"*70)
            print("SKELETON QUALITY METRICS:", skeleton_metrics)
            print("-"*70)
            
            # ä¿å­˜éª¨æ¶ä¿¡æ¯
            skeleton_info = {
                'skeleton_matrix': skeleton.tolist(),
                'pc_sets': {self.variable_list[k]: [self.variable_list[v] for v in vals] 
                           for k, vals in pc_sets.items()},
                'allowed_edges': self.skeleton_constraints['allowed_edges'],
                'forbidden_pairs': self.skeleton_constraints['forbidden_pairs'],
                'n_edges': int(skeleton.sum() // 2),
                'alpha': skeleton_alpha,
                'max_cond_size': skeleton_max_cond_size,
                'variable_type': variable_type,
                'metrics': skeleton_metrics  # æ·»åŠ æŒ‡æ ‡
            }
            
            skeleton_path = os.path.join(self.output_dir, "skeleton_info.json")
            with open(skeleton_path, 'w') as f:
                json.dump(skeleton_info, f, indent=2)
            
            print(f"\nâœ“ Skeleton saved to {skeleton_path}")
            print(f"  Allowed edges: {len(self.skeleton_constraints['allowed_edges'])}")
            print(f"  Forbidden pairs: {len(self.skeleton_constraints['forbidden_pairs'])}")
            print("="*70 + "\n")
            exit()
        
        # ===== åŠ è½½åŸºçº¿å‚è€ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰=====
        if self.use_baseline_reference:
            self.baseline_structured_graphs = load_baseline_reference_from_predict(
                dataset_name=self.domain_name,
                variable_list=self.variable_list,
                predict_dir=self.baseline_predict_dir,
                methods=self.baseline_methods,
                top_k=self.baseline_top_k,
                threshold=self.baseline_threshold
            )
            # ç”Ÿæˆä¾›LLMé˜…è¯»çš„æ–‡æœ¬æè¿°
            self.baseline_reference_text = self._format_baseline_reference_text()

        # ===== åˆå§‹åŒ–NOTEARSä¼˜åŒ–å™¨ï¼ˆæ”¯æŒMLP scoreï¼‰=====
        self.use_notears_refinement = use_notears_refinement
        self.notears_start_iter = notears_start_iter
        self.notears_use_mlp = notears_use_mlp  # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        
        if self.use_notears_refinement:
            if self.notears_use_mlp:
                # ä½¿ç”¨MLP-based NOTEARSï¼ˆæ¨èï¼‰
                from classical_refinement import NOTEARSMLPRefiner
                self.notears_refiner = NOTEARSMLPRefiner(
                    w_threshold=notears_threshold,
                    max_iter=100,
                    h_tol=1e-8,
                    rho_max=1e+16,
                    w_lr=0.001,
                    hidden_dims=[10, 1],
                    device=self.device
                )
                print("\n" + "="*70)
                print("NOTEARS-MLP REFINEMENT ENABLED (Official Implementation)")
                print("="*70)
                print(f"Using continuous optimization with MLP")
                print(f"Weight threshold: {notears_threshold}")
                print(f"Max iterations: 100")
                print(f"Learning rate: 0.001")
                print(f"Hidden dims: [10, 1]")
                print(f"Start from iteration: {notears_start_iter}")
                print("="*70 + "\n")
            else:
                # ä½¿ç”¨ä¼ ç»Ÿçš„å¤šé¡¹å¼Ridgeå›å½’NOTEARS
                from classical_refinement import NOTEARSRefiner
                self.notears_refiner = NOTEARSRefiner(
                    alpha=notears_alpha,
                    w_threshold=notears_threshold,
                    max_iter=50,
                    poly_degree=notears_poly_degree
                )
                print("\n" + "="*70)
                print("NOTEARS REFINEMENT ENABLED (Polynomial Ridge Regression)")
                print("="*70)
                print(f"Alpha (L2 regularization): {notears_alpha}")
                print(f"Threshold: {notears_threshold}")
                print(f"Polynomial degree: {notears_poly_degree} ({'linear' if notears_poly_degree==1 else 'nonlinear'})")
                print(f"Start from iteration: {notears_start_iter}")
                print("="*70 + "\n")
        
        # ===== åˆå§‹åŒ–è´ªå¿ƒä¼˜åŒ–å™¨ï¼ˆæ¨èï¼‰=====
        self.use_greedy_refinement = use_greedy_refinement
        self.greedy_start_iter = greedy_start_iter
        if self.use_greedy_refinement:
            from greedy_refinement import GreedyGraphRefiner
            self.greedy_refiner = GreedyGraphRefiner(
                max_modifications=greedy_max_modifications,
                min_improvement=greedy_min_improvement,
                eval_epochs=greedy_eval_epochs,
                max_candidates_per_type=greedy_max_candidates,
                allow_add=True,
                allow_delete=True,
                allow_reverse=True
            )
            print("\n" + "="*70)
            print("GREEDY GRAPH REFINEMENT ENABLED (MLP-based)")
            print("="*70)
            print(f"Max modifications per iteration: {greedy_max_modifications}")
            print(f"Min LL improvement threshold: {greedy_min_improvement}")
            print(f"Evaluation epochs: {greedy_eval_epochs}")
            print(f"Max candidates per operation type: {greedy_max_candidates}")
            print(f"Start from iteration: {greedy_start_iter}")
            print("="*70 + "\n")
        
        # ===== å­˜å‚¨MCTSå‚æ•° =====
        self.mcts_simulations = mcts_simulations
        self.mcts_exploration_weight = mcts_exploration_weight
        self.mcts_max_depth = mcts_max_depth
        
        if llm_type == "local":
            # æ£€æŸ¥æ˜¯å¦æä¾›äº†é¢„åŠ è½½çš„æ¨¡å‹
            assert shared_model is not None, "shared_model must be provided when llm_type='local'"
            self.tokenizer = shared_tokenizer
            self.model = shared_model
            
            # ä½¿ç”¨å…±äº«çš„æ¨¡å‹åˆå§‹åŒ–å„æ¨¡å—
            self.hypothesis_generator = LLMHypothesisGenerator(
                model_type="local",
                shared_tokenizer=self.tokenizer,
                shared_model=self.model
            )
            
            self.post_processor = PostProcessor(
                model_type="local",
                tokenizer=self.tokenizer,
                model=self.model
            )
            
        else:  # openai
            print(f"Using OpenAI-compatible API")
            if openai_base_url:
                print(f"Base URL: {openai_base_url}")
            
            # OpenAIä¸éœ€è¦é¢„åŠ è½½æ¨¡å‹
            self.tokenizer = None
            self.model = None
            
            self.hypothesis_generator = LLMHypothesisGenerator(
                model_type="openai",
                base_url=openai_base_url,
                api_key=openai_api_key
            )
            
            self.post_processor = PostProcessor(
                model_type="openai",
                base_url=openai_base_url,
                api_key=openai_api_key
            )
        
        # æ¨¡å‹æ‹Ÿåˆå¼•æ“ï¼ˆä¸éœ€è¦LLMï¼‰
        self.fitting_engine = ModelFittingEngine(device=device)
        
        # ===== å¹²é¢„æµ‹è¯•å¼•æ“ =====
        # æ³¨æ„ï¼šå·²ç»åœ¨ __init__ å‰æœŸåˆå§‹åŒ–äº† self.use_intervention_test å’Œ self.accumulated_evidence
        self.num_intervention_experiments = num_intervention_experiments
        self.policy_verifier = EvidencePolicyVerifier() # å§‹ç»ˆåˆå§‹åŒ–ï¼Œç”¨äºéªŒè¯ LLM çš„å†³ç­–
        if self.dataset.interventions is not None:
            self.intervention_tester = InterventionTester(self.dataset)
        else:
            self.use_intervention_test = False
            self.intervention_tester = None
            if use_intervention_test:
                print("[Warning] Intervention test requested but no intervention data found. Disabling.")
        
        # å­˜å‚¨å†å²
        self.iteration_history = []
        
        print("âœ“ Pipeline initialized successfully!")
        print("="*70 + "\n")

    def _format_baseline_reference_text(self) -> str:
        """å°†ç»“æ„åŒ–åŸºçº¿å›¾è½¬æ¢ä¸ºä¾›LLMé˜…è¯»çš„æ–‡æœ¬æè¿°"""
        if not self.baseline_structured_graphs:
            return ""
            
        text = "\n" + "="*50 + "\n"
        text += "ğŸ“Š STATISTICAL BASELINE REFERENCE (from traditional methods):\n"
        text += "="*50 + "\n"
        text += "The following causal relationships were suggested by traditional algorithms.\n"
        text += "Use them as hints, but rely on your domain knowledge and interventional evidence.\n\n"
        
        for method, graph in self.baseline_structured_graphs.items():
            method_name = method.upper()
            text += f"[{method_name} Algorithm]:\n"
            edges = []
            for node in graph['nodes']:
                for parent in node.get('parents', []):
                    edges.append(f"  - {parent} â†’ {node['name']}")
            
            if edges:
                text += "\n".join(edges) + "\n"
            else:
                text += "  (no edges predicted)\n"
            text += "\n"
            
        return text
    
    def _save_dataset_info(self):
        """ä¿å­˜æ•°æ®é›†ä¿¡æ¯(ä»…å½“æœ‰datasetæ—¶)"""
        info = {
            "domain": self.dataset.domain_name,
            "n_variables": self.dataset.n_variables,
            "n_samples_total": self.dataset.n_samples,
            "n_samples_used": len(self.data),
            "variable_names": self.dataset.variable_names,
            "ground_truth_edges": self.dataset.get_ground_truth_edges(),
            "intervention_summary": self.dataset.get_intervention_summary()
        }
        
        info_path = os.path.join(self.output_dir, "dataset_info.json")
        # with open(info_path, 'w') as f:
        #     json.dump(info, f, indent=2)
        # print(f"[Info] Dataset info saved to {info_path}") # TODO: currently not used
    
    def run(
        self,
        num_iterations: Optional[int] = None,
        iterations_per_node: float = 3.0,  # æ¯èŠ‚ç‚¹å»ºè®®è¿­ä»£æ¬¡æ•°
        early_stopping_patience: int = 3,   # è¿ç»­å¤šå°‘æ¬¡æ”¹è¿›å¤±è´¥åˆ™åœæ­¢
        num_epochs: int = 100,
        learning_rate: float = 0.01,
        temperature: float = 0.6,
        llm_model_name: str = "gpt-4o",
        max_tokens: int = 4096,
        verbose: bool = True,
        use_hill_climbing: bool = True,  # ä¿æŒåŸæœ‰å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
        use_mcts: bool = False,  # MCTSç­–ç•¥å¼€å…³
        acceptance_tolerance: float = 0.0,
        max_retries: int = 10,
        use_local_amendment: bool = True,
        llm_only: bool = False,
        choose_best: bool = False,
        use_intervention_test: Optional[bool] = None  # æ–°å¢
    ) -> Dict:
        """è¿è¡Œå®Œæ•´çš„CMAæµç¨‹ï¼ˆå†…éƒ¨å·²é‡æ„ï¼Œæ”¯æŒç­–ç•¥æ‰©å±•ï¼‰"""
        
        if use_intervention_test is not None:
            self.use_intervention_test = use_intervention_test
        
        # è‡ªåŠ¨è®¡ç®—è¿­ä»£æ¬¡æ•°
        if num_iterations is None:
            num_iterations = int(len(self.variable_list) * iterations_per_node)
            print(f"[Info] num_iterations is None, auto-calculated: {len(self.variable_list)} nodes * {iterations_per_node} = {num_iterations}")
        
        # å†…éƒ¨è½¬æ¢ï¼šæ ¹æ®å‚æ•°é€‰æ‹©ç­–ç•¥
        if use_mcts:
            search_strategy = "mcts"
        else:
            search_strategy = "hill_climbing"
        
        print("\n" + "="*70)
        print(f"STARTING CMA PIPELINE: {self.domain_name.upper()}")
        print(f"Total iterations: {num_iterations}")
        print(f"Early stopping patience: {early_stopping_patience}")
        print(f"Choose best initial (Baseline vs LLM): {choose_best}")
        print("="*70)
        
        if search_strategy == "hill_climbing":
            strategy = HillClimbingStrategy(
                pipeline=self,
                acceptance_tolerance=acceptance_tolerance
            )
        elif search_strategy == "mcts":
            strategy = MCTSStrategy(
                pipeline=self,
                num_simulations=self.mcts_simulations,
                exploration_weight=self.mcts_exploration_weight,
                max_depth=self.mcts_max_depth
            )
        else:
            raise ValueError(f"Unknown search strategy: {search_strategy}")
        
        # ===== æ‰§è¡Œæœç´¢ =====
        results = strategy.search(
            num_iterations=num_iterations,
            early_stopping_patience=early_stopping_patience,
            num_epochs=num_epochs,
            learning_rate=learning_rate,
            temperature=temperature,
            llm_model_name=llm_model_name,
            max_tokens=max_tokens,
            verbose=verbose,
            max_retries=max_retries,
            use_local_amendment=use_local_amendment,
            llm_only=llm_only,
            choose_best=choose_best
        )
        
        # ===== æ›´æ–°å†å²è®°å½• =====
        self.iteration_history = strategy.iteration_history

    def get_best_graph(self) -> Dict:
        """è¿”å›æ‹Ÿåˆåº¦æœ€å¥½çš„å›¾ï¼ˆæ”¯æŒçˆ¬å±±ç­–ç•¥ï¼‰"""
        if not self.iteration_history:
            return None, float('-inf')
        
        best_idx = max(range(len(self.iteration_history)),
                       key=lambda i: self.iteration_history[i]['results']['log_likelihood'])
        return self.iteration_history[best_idx]['graph'], self.iteration_history[best_idx]['results']['log_likelihood']
    
    def get_accepted_graphs(self) -> List[Dict]:
        """è¿”å›æ‰€æœ‰è¢«æ¥å—çš„å›¾ï¼ˆä»…çˆ¬å±±ç­–ç•¥æœ‰æ•ˆï¼‰"""
        return [h for h in self.iteration_history if h.get('accepted', True)]


# ========== æ‰¹é‡å®éªŒè¿è¡Œå™¨ ==========
class BatchExperimentRunner:
    """æ‰¹é‡è¿è¡ŒCMAå®éªŒ"""
    
    def __init__(
        self,
        csv_config_path: str,
        base_output_dir: str = "./cma_experiments",
        # LLMé…ç½®
        llm_type: str = "openai",
        llm_model_path: str = None,
        openai_base_url: str = None,
        openai_api_key: str = None
    ):
        self.csv_config_path = csv_config_path
        self.base_output_dir = base_output_dir
        self.llm_type = llm_type
        self.llm_model_path = llm_model_path
        self.openai_base_url = openai_base_url
        self.openai_api_key = openai_api_key
        
        # é¢„åŠ è½½çš„æ¨¡å‹ï¼ˆç”¨äºæ‰¹é‡å®éªŒå¤ç”¨ï¼‰
        self.shared_tokenizer = None
        self.shared_model = None
        
        os.makedirs(base_output_dir, exist_ok=True)
    
    def _preload_local_model(self):
        """é¢„åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼Œä¾›æ‰€æœ‰å®éªŒå¤ç”¨ï¼‰"""
        from vllm import LLM
        from transformers import AutoTokenizer
        
        print(f"Loading local model from {self.llm_model_path}... This may take a few minutes...")
        
        # ä½¿ç”¨ vLLM åŠ è½½æ¨¡å‹
        self.shared_model = LLM(
            model=self.llm_model_path,
            dtype="bfloat16",
            tensor_parallel_size=1,
            gpu_memory_utilization=0.9,
            trust_remote_code=True,
        )
        
        # åŠ è½½tokenizer
        self.shared_tokenizer = AutoTokenizer.from_pretrained(self.llm_model_path)
        
        print(f"\nâœ“ Model pre-loaded successfully!")
        print("="*70 + "\n")
    
    def run_all_experiments(
        self,
        split: str = "test",
        num_runs: int = 1,  # æ–°å¢: æ¯ä¸ªæ•°æ®é›†è·‘å¤šå°‘æ¬¡ä»¥è¿›è¡Œæ˜¾è‘—æ€§æµ‹è¯•
        num_iterations: Optional[int] = None,
        iterations_per_node: float = 3.0,
        early_stopping_patience: int = 3,
        num_epochs: int = 50,
        device: str = "cpu",
        use_skeleton: bool = False,
        skeleton_alpha: float = 0.05,
        skeleton_max_cond_size: int = 3,
        use_notears_refinement: bool = False,  # NOTEARSä¼˜åŒ–
        notears_use_mlp: bool = False,  # NOTEARSæ˜¯å¦ä½¿ç”¨MLPä½œä¸ºscore
        notears_alpha: float = 0.001,  # L2æ­£åˆ™åŒ–ï¼ˆä»…Ridgeç‰ˆæœ¬ï¼‰
        notears_threshold: float = 0.15,  # è¾¹æƒé‡é˜ˆå€¼
        notears_poly_degree: int = 2,  # å¤šé¡¹å¼é˜¶æ•°ï¼ˆä»…Ridgeç‰ˆæœ¬ï¼‰
        notears_start_iter: int = 0,
        use_greedy_refinement: bool = False,  # è´ªå¿ƒä¼˜åŒ–ï¼ˆæ¨èï¼‰
        greedy_max_modifications: int = 10,
        greedy_min_improvement: float = 0.01,
        greedy_eval_epochs: int = 15,
        greedy_max_candidates: int = 30,
        greedy_start_iter: int = 0,
        mcts_simulations: int = 50,  # MCTSå‚æ•°
        mcts_exploration_weight: float = 1.414,
        mcts_max_depth: int = 5,
        llm_only: bool = False,
        choose_best: bool = False,
        use_intervention_test: bool = False,
        num_intervention_experiments: int = 3,
        **kwargs
    ):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        
        # åŠ è½½æ‰€æœ‰æ•°æ®é›†
        print(f"Loading datasets from {self.csv_config_path}...")
        datasets = DataLoader.load_all_from_csv(self.csv_config_path, split=split)
        
        print(f"\n{'='*70}")
        print(f"BATCH EXPERIMENT: {len(datasets)} datasets loaded")
        print(f"Device: {device}")
        print(f"{'='*70}\n")
        
        # é¢„åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨localæ¨¡å¼ï¼‰
        if self.llm_type == "local":
            self._preload_local_model()
        
        results_summary = []
        
        for idx, dataset in enumerate(datasets):
            print(f"\n{'#'*70}")
            print(f"EXPERIMENT {idx+1}/{len(datasets)}: {dataset.domain_name}")
            print(f"Running {num_runs} independent trials for significance testing")
            print(f"{'#'*70}\n")
            
            # å­˜å‚¨è¯¥æ•°æ®é›†çš„æ‰€æœ‰è¿è¡Œç»“æœ
            dataset_run_results = []
            
            for run_idx in range(num_runs):
                print(f"\n>>> Trial {run_idx+1}/{num_runs}")
                
                # ä¸ºæ¯ä¸ªè¿è¡Œåˆ›å»ºå­ç›®å½•
                trial_output_dir = os.path.join(self.base_output_dir, f"{idx:02d}_{dataset.domain_name}", f"run_{run_idx:02d}")
                os.makedirs(trial_output_dir, exist_ok=True)
                
                # ä¿å­˜å®Œæ•´é…ç½®
                config_path = os.path.join(trial_output_dir, "config.json")
                # æ”¶é›†æ‰€æœ‰é…ç½®
                full_config = {
                    "base_output_dir": self.base_output_dir,
                    "llm_type": self.llm_type,
                    "llm_model_path": self.llm_model_path,
                    "dataset": dataset.domain_name,
                    "run_idx": run_idx,
                    "split": split,
                    "num_runs": num_runs,
                    "num_iterations": num_iterations,
                    "iterations_per_node": iterations_per_node,
                    "early_stopping_patience": early_stopping_patience,
                    "num_epochs": num_epochs,
                    "device": device,
                    "use_skeleton": use_skeleton,
                    "skeleton_alpha": skeleton_alpha,
                    "skeleton_max_cond_size": skeleton_max_cond_size,
                    "use_notears_refinement": use_notears_refinement,
                    "notears_use_mlp": notears_use_mlp,
                    "use_greedy_refinement": use_greedy_refinement,
                    "greedy_max_modifications": greedy_max_modifications,
                    "greedy_min_improvement": greedy_min_improvement,
                    "greedy_eval_epochs": greedy_eval_epochs,
                    "greedy_max_candidates": greedy_max_candidates,
                    "greedy_start_iter": greedy_start_iter,
                    "mcts_simulations": mcts_simulations,
                    "mcts_exploration_weight": mcts_exploration_weight,
                    "mcts_max_depth": mcts_max_depth,
                    "choose_best": choose_best,
                    "use_intervention_test": use_intervention_test,
                    "num_intervention_experiments": num_intervention_experiments,
                    **kwargs
                }
                with open(config_path, 'w') as f:
                    json.dump(full_config, f, indent=2, ensure_ascii=False)
                
                # è¿è¡ŒCMA
                try:
                    pipeline = CMAPipeline(
                        dataset=dataset,
                        output_dir=trial_output_dir,
                        use_observational_only=True,
                        device=device,
                        use_skeleton=use_skeleton,
                        skeleton_alpha=skeleton_alpha,
                        skeleton_max_cond_size=skeleton_max_cond_size,
                        use_notears_refinement=use_notears_refinement,  # NOTEARSå‚æ•°
                        notears_use_mlp=notears_use_mlp,
                        notears_alpha=notears_alpha,
                        notears_threshold=notears_threshold,
                        notears_poly_degree=notears_poly_degree,
                        notears_start_iter=notears_start_iter,
                        use_greedy_refinement=use_greedy_refinement,  # è´ªå¿ƒå‚æ•°
                        greedy_max_modifications=greedy_max_modifications,
                        greedy_min_improvement=greedy_min_improvement,
                        greedy_eval_epochs=greedy_eval_epochs,
                        greedy_max_candidates=greedy_max_candidates,
                        greedy_start_iter=greedy_start_iter,
                        mcts_simulations=mcts_simulations,  # MCTSå‚æ•°
                        mcts_exploration_weight=mcts_exploration_weight,
                        mcts_max_depth=mcts_max_depth,
                        use_baseline_reference=kwargs.get('use_baseline_reference', False),  # åŸºçº¿å‚è€ƒå‚æ•°
                        baseline_predict_dir=kwargs.get('baseline_predict_dir', 'predict'),
                        baseline_methods=kwargs.get('baseline_methods', ['corr', 'invcov']),
                        baseline_top_k=kwargs.get('baseline_top_k', 10),
                        baseline_threshold=kwargs.get('baseline_threshold', 0.5),
                        choose_best=choose_best,
                        use_intervention_test=use_intervention_test,
                        num_intervention_experiments=num_intervention_experiments,
                        llm_type=self.llm_type,
                        llm_model_path=self.llm_model_path,
                        openai_base_url=self.openai_base_url,
                        openai_api_key=self.openai_api_key,
                        shared_tokenizer=self.shared_tokenizer,
                        shared_model=self.shared_model
                    )
                    
                    # ç§»é™¤ baseline å‚æ•°ï¼Œå› ä¸ºå®ƒä»¬å·²ç»åœ¨ __init__ ä¸­ä½¿ç”¨äº†
                    run_kwargs = {k: v for k, v in kwargs.items() 
                                 if k not in ['use_baseline_reference', 'baseline_predict_dir', 
                                             'baseline_methods', 'baseline_top_k', 
                                             'baseline_threshold', 'choose_best']}
                    
                    pipeline.run(
                        num_iterations=num_iterations,
                        iterations_per_node=iterations_per_node,
                        early_stopping_patience=early_stopping_patience,
                        num_epochs=num_epochs,
                        llm_only=llm_only,
                        choose_best=choose_best,
                        **run_kwargs
                    )
                    
                    # æå–è¯„ä¼°æŒ‡æ ‡(å¦‚æœæœ‰ground truth)
                    final_graph, best_ll = pipeline.get_best_graph()
                    metrics = _compute_metrics(pipeline, final_graph)
                    
                    # ä¿å­˜è¯¥æ¬¡è¿è¡Œçš„è¯¦ç»†å†å²å’Œç»“æœåˆ°æœ¬åœ°ç›®å½•
                    trial_history_path = os.path.join(trial_output_dir, "run_history.json")
                    with open(trial_history_path, 'w') as f:
                        json.dump(pipeline.iteration_history, f, indent=2)
                    
                    trial_summary_path = os.path.join(trial_output_dir, "run_summary.json")
                    trial_summary = {
                        "run_id": run_idx,
                        "domain": dataset.domain_name,
                        "final_ll": best_ll,
                        "final_bic": final_graph['metadata'].get('bic'),
                        "num_edges_predicted": final_graph['metadata']['num_edges'],
                        "num_edges_true": int(dataset.ground_truth_graph.sum()),
                        "metrics": metrics,
                        "status": "success"
                    }
                    with open(trial_summary_path, 'w') as f:
                        json.dump(trial_summary, f, indent=2)
                    
                    # æå–éª¨æ¶æŒ‡æ ‡ï¼ˆå¦‚æœä½¿ç”¨äº†éª¨æ¶ï¼‰
                    skeleton_metrics = None
                    skeleton_info_path = os.path.join(trial_output_dir, "skeleton_info.json")
                    if os.path.exists(skeleton_info_path):
                        with open(skeleton_info_path, 'r') as f:
                            skeleton_info = json.load(f)
                            skeleton_metrics = skeleton_info.get('metrics', {})
                    
                    dataset_run_results.append({
                        "run_id": run_idx,
                        "status": "success",
                        "final_ll": best_ll,
                        "num_edges_predicted": final_graph['metadata']['num_edges'],
                        "num_edges_true": int(dataset.ground_truth_graph.sum()),
                        "metrics": metrics,
                        "skeleton_metrics": skeleton_metrics,
                        "iteration_history": pipeline.iteration_history
                    })
                    
                    print(f"âœ… Trial {run_idx+1} completed")
                    
                except Exception as e:
                    print(f"âŒ Trial {run_idx+1} failed: {e}")
                    import traceback
                    traceback.print_exc()
                    
                    # è®°å½•å¤±è´¥ä¿¡æ¯åˆ°æœ¬åœ°ç›®å½•
                    trial_summary_path = os.path.join(trial_output_dir, "run_summary.json")
                    with open(trial_summary_path, 'w') as f:
                        json.dump({
                            "run_id": run_idx,
                            "domain": dataset.domain_name,
                            "status": "failed",
                            "error": str(e)
                        }, f, indent=2)
                        
                    dataset_run_results.append({
                        "run_id": run_idx,
                        "status": "failed",
                        "error": str(e)
                    })
            
            # --- æ±‡æ€»è¯¥æ•°æ®é›†çš„æ‰€æœ‰è¿è¡Œç»“æœ ---
            success_runs = [r for r in dataset_run_results if r['status'] == 'success']
            if success_runs:
                # æå–æŒ‡æ ‡è¿›è¡Œç»Ÿè®¡
                metrics_to_stats = ['f1_score', 'shd', 'precision', 'recall']
                stats_summary = {}
                
                for m in metrics_to_stats:
                    vals = [r['metrics'].get(m, 0) for r in success_runs]
                    stats_summary[f"{m}_mean"] = float(np.mean(vals))
                    stats_summary[f"{m}_std"] = float(np.std(vals))
                
                # æœ€ç»ˆLLç»Ÿè®¡
                ll_vals = [r['final_ll'] for r in success_runs if r['final_ll'] != float('-inf')]
                if ll_vals:
                    stats_summary["ll_mean"] = float(np.mean(ll_vals))
                    stats_summary["ll_std"] = float(np.std(ll_vals))
                
                results_summary.append({
                    "experiment_id": idx,
                    "domain": dataset.domain_name,
                    "status": "success",
                    "num_runs": num_runs,
                    "success_runs": len(success_runs),
                    "stats": stats_summary,
                    # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™ä¸€ä¸ªâ€œä»£è¡¨æ€§â€ç»“æœï¼ˆå–ç¬¬ä¸€ä¸ªæˆåŠŸçš„è¿è¡Œï¼‰
                    "final_ll": success_runs[0]['final_ll'],
                    "num_edges_predicted": success_runs[0]['num_edges_predicted'],
                    "num_edges_true": success_runs[0]['num_edges_true'],
                    "metrics": success_runs[0]['metrics'],
                    "skeleton_metrics": success_runs[0]['skeleton_metrics'],
                    "output_dir": os.path.join(self.base_output_dir, f"{idx:02d}_{dataset.domain_name}"),
                    "all_runs": dataset_run_results
                })
            else:
                results_summary.append({
                    "experiment_id": idx,
                    "domain": dataset.domain_name,
                    "status": "failed",
                    "error": "All runs failed",
                    "output_dir": os.path.join(self.base_output_dir, f"{idx:02d}_{dataset.domain_name}")
                })
        
        # ä¿å­˜æ€»ç»“
        summary_path = os.path.join(self.base_output_dir, "experiments_summary.json")
        with open(summary_path, 'w') as f:
            json.dump(results_summary, f, indent=2)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_summary(results_summary)
        
        return results_summary
    
    def _print_summary(self, results_summary: List[Dict]):
        """æ‰“å°æ€»ç»“ä¿¡æ¯"""
        
        print(f"\n{'='*70}")
        print("BATCH EXPERIMENTS SUMMARY")
        print(f"{'='*70}\n")
        
        success_count = sum(1 for r in results_summary if r['status'] == 'success')
        failed_count = len(results_summary) - success_count
        
        print(f"Total experiments: {len(results_summary)}")
        print(f"  Success: {success_count}")
        print(f"  Failed: {failed_count}")
        
        if success_count > 0:
            # è¡¨æ ¼1: æ¯ä¸ªå®éªŒçš„è¿­ä»£å†å²
            for r in results_summary:
                if r['status'] == 'success':
                    print(f"\n{'-'*70}")
                    print(f"Iteration History - Experiment {r['experiment_id']} ({r['domain']}):")
                    print(f"{'-'*70}")
                    print(f"{'Iter':<6} {'Status':<8} {'SHD':<8} {'LL':<12} {'BIC':<12} {'Edges':<8} {'F1':<8} {'Precision':<10} {'Recall':<8}")
                    print(f"{'-'*70}")
                    
                    iteration_history = r.get('iteration_history', [])
                    for iteration in iteration_history:
                        # æ£€æŸ¥æ˜¯å¦è¢«æ¥å—
                        accepted = iteration.get('accepted', True)
                        status_str = "âœ“" if accepted else "âœ—"
                        
                        shd = iteration['metrics'].get('shd', None) if iteration.get('metrics') else None
                        ll = iteration['results'].get('log_likelihood', None)
                        bic = iteration['results'].get('bic', None)
                        edges = iteration['results'].get('num_edges', iteration.get('graph', {}).get('metadata', {}).get('num_edges', 0) if iteration.get('graph') else 0)
                        f1 = iteration['metrics'].get('f1_score', None) if iteration.get('metrics') else None
                        precision = iteration['metrics'].get('precision', None) if iteration.get('metrics') else None
                        recall = iteration['metrics'].get('recall', None) if iteration.get('metrics') else None
                        
                        # to
                        shd_str = f"{shd:.1f}" if shd is not None else "N/A"
                        ll_str = f"{ll:.4f}" if ll is not None and ll != float('-inf') else "N/A"
                        bic_str = f"{bic:.2f}" if bic is not None else "N/A"
                        edges_str = f"{edges}" if edges is not None else "N/A"
                        f1_str = f"{f1:.4f}" if f1 is not None else "N/A"
                        precision_str = f"{precision:.4f}" if precision is not None else "N/A"
                        recall_str = f"{recall:.4f}" if recall is not None else "N/A"
                        
                        print(f"{iteration['iteration']:<6} "
                              f"{status_str:<8} "
                              f"{shd_str:<8} "
                              f"{ll_str:<12} "
                              f"{bic_str:<12} "
                              f"{edges_str:<8} "
                              f"{f1_str:<8} "
                              f"{precision_str:<10} "
                              f"{recall_str:<8}")
            
            # è¡¨æ ¼2: æœ€ç»ˆç»“æœ (åŒ…å«ç»Ÿè®¡ä¿¡æ¯)
            print(f"\n{'-'*70}")
            print("Final Results (Mean Â± Std across trials):")
            print(f"{'-'*70}")
            print(f"{'ID':<5} {'Domain':<15} {'F1-Score':<20} {'SHD':<15} {'Precision':<15} {'Recall':<15}")
            print(f"{'-'*70}")
            
            for r in results_summary:
                if r['status'] == 'success':
                    s = r.get('stats', {})
                    f1_str = f"{s.get('f1_score_mean', 0):.4f}Â±{s.get('f1_score_std', 0):.4f}"
                    shd_str = f"{s.get('shd_mean', 0):.2f}Â±{s.get('shd_std', 0):.2f}"
                    prec_str = f"{s.get('precision_mean', 0):.4f}Â±{s.get('precision_std', 0):.4f}"
                    rec_str = f"{s.get('recall_mean', 0):.4f}Â±{s.get('recall_std', 0):.4f}"
                    
                    print(f"{r['experiment_id']:<5} "
                          f"{r['domain']:<15} "
                          f"{f1_str:<20} "
                          f"{shd_str:<15} "
                          f"{prec_str:<15} "
                          f"{rec_str:<15}")
            
            # è¡¨æ ¼3: éª¨æ¶è´¨é‡ï¼ˆå¦‚æœæœ‰ï¼‰
            has_skeleton = any(r.get('skeleton_metrics') for r in results_summary if r['status'] == 'success')
            if has_skeleton:
                print(f"\n{'-'*70}")
                print("Skeleton Quality (before LLM):")
                print(f"{'-'*70}")
                print(f"{'ID':<5} {'Domain':<15} {'Skeleton F1':<12} {'Skeleton SHD':<12} {'Precision':<12} {'Recall':<12}")
                print(f"{'-'*70}")
                
                for r in results_summary:
                    if r['status'] == 'success' and r.get('skeleton_metrics'):
                        skel = r['skeleton_metrics']
                        print(f"{r['experiment_id']:<5} "
                              f"{r['domain']:<15} "
                              f"{skel.get('f1_score', 0):<12.4f} "
                              f"{skel.get('shd', 0):<12} "
                              f"{skel.get('precision', 0):<12.4f} "
                              f"{skel.get('recall', 0):<12.4f}")
        
        if failed_count > 0:
            print(f"\n{'-'*70}")
            print("Failed Experiments:")
            print(f"{'-'*70}")
            for r in results_summary:
                if r['status'] == 'failed':
                    print(f"  {r['experiment_id']}: {r['domain']} - {r['error'][:50]}")
        
        print(f"\n{'='*70}\n")

# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
if __name__ == "__main__":
    
    import argparse
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='CMA Pipeline - Causal Discovery')
    parser.add_argument('--mode', type=str, default='batch', choices=['single', 'batch', 'llm-only'],
                       help='è¿è¡Œæ¨¡å¼: single(å•ä¸ªå®éªŒ), batch(æ‰¹é‡å®éªŒ) æˆ– llm-only(ä»…LLMç”Ÿæˆä¸€æ¬¡)')
    parser.add_argument('--llm_type', type=str, default='local', choices=['local', 'openai'],
                       help='LLMç±»å‹: local æˆ– openai')
    parser.add_argument('--model_path', type=str, 
                       default='/mnt/shared-storage-user/safewt-share/HuggingfaceModels/Qwen3-14B',
                       help='æœ¬åœ°æ¨¡å‹è·¯å¾„(llm_type=localæ—¶ä½¿ç”¨)')
    parser.add_argument('--openai_url', type=str,
                       default='http://35.220.164.252:3888/v1/',
                       help='OpenAI API URL')
    parser.add_argument('--openai_key', type=str,
                       default='sk-x1DLgF9tW1t2IwCrUFyCfIIYFookGgO4qseCxb9vefNHQPcp',
                       help='OpenAI API key')
    parser.add_argument('--csv_path', type=str,
                       default='/mnt/shared-storage-user/pengbo/created/projects/CDLLM/Test-1213/real_test.csv',
                       help='æ‰¹é‡å®éªŒçš„CSVé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str,
                       default='./cma_experiments',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--num_iterations', type=int, default=None,
                       help='CMAè¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ä¸ºNoneï¼Œç”±iterations_per_nodeè®¡ç®—ï¼‰')
    parser.add_argument('--iterations_per_node', type=float, default=1.0,
                       help='å½“num_iterationsä¸ºNoneæ—¶ï¼Œæ¯èŠ‚ç‚¹åˆ†é…çš„è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--early_stopping_patience', type=int, default=5,
                       help='æ—©åœè€å¿ƒå€¼ï¼šè¿ç»­å¤šå°‘æ¬¡å›¾ä¿®æ”¹æœªè¢«æ¥å—åˆ™åœæ­¢è¿­ä»£')
    parser.add_argument('--num_runs', type=int, default=1,
                       help='æ˜¾è‘—æ€§æµ‹è¯•çš„ç‹¬ç«‹è¿è¡Œæ¬¡æ•°')
    parser.add_argument('--num_epochs', type=int, default=50,
                       help='æ¨¡å‹æ‹Ÿåˆçš„epochæ•°')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡: cpu æˆ– cuda')
    parser.add_argument('--use_hill_climbing', action='store_true',
                       help='å¯ç”¨çˆ¬å±±ç­–ç•¥(åŸºäºLLæ¥å—/æ‹’ç»å›¾ä¿®æ”¹)')
    parser.add_argument('--acceptance_tolerance', type=float, default=0.0,
                       help='çˆ¬å±±ç­–ç•¥çš„æ¥å—èŒƒå›´: new_ll >= best_ll - tolerance')
    
    # MCTSå‚æ•°
    parser.add_argument('--use_mcts', action='store_true',
                       help='ä½¿ç”¨MCTSæœç´¢ç­–ç•¥ï¼ˆä¸use_hill_climbingäº’æ–¥ï¼‰')
    parser.add_argument('--mcts_simulations', type=int, default=50,
                       help='MCTSæ¯æ¬¡è¿­ä»£çš„æ¨¡æ‹Ÿæ¬¡æ•°')
    parser.add_argument('--mcts_exploration_weight', type=float, default=1.414,
                       help='MCTSçš„UCB1æ¢ç´¢æƒé‡ï¼ˆsqrt(2)â‰ˆ1.414ï¼‰')
    parser.add_argument('--mcts_max_depth', type=int, default=5,
                       help='MCTSçš„æœ€å¤§æœç´¢æ·±åº¦')
    parser.add_argument('--use_skeleton', action='store_true',
                       help='å¯ç”¨MMHCéª¨æ¶æ„å»ºï¼Œç”¨ç»Ÿè®¡æ–¹æ³•ç¼©å°æœç´¢ç©ºé—´')
    parser.add_argument('--skeleton_alpha', type=float, default=0.05,
                       help='éª¨æ¶æ„å»ºçš„ç‹¬ç«‹æ€§æ£€éªŒæ˜¾è‘—æ€§æ°´å¹³')
    parser.add_argument('--skeleton_max_cond_size', type=int, default=3,
                       help='éª¨æ¶æ„å»ºçš„æœ€å¤§æ¡ä»¶é›†å¤§å°')
    parser.add_argument('--verbose', action='store_true',
                       help='')
    parser.add_argument('--max_retries', type=int, default=10,
                       help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    
    # NOTEARSå‚æ•°
    parser.add_argument('--use_notears_refinement', action='store_true',
                       help='ä½¿ç”¨NOTEARSä¼˜åŒ–')
    parser.add_argument('--notears_use_mlp', action='store_true',
                       help='NOTEARSä½¿ç”¨MLPä½œä¸ºscoreï¼ˆæ¨èï¼Œæ›´å‡†ç¡®ï¼‰')
    
    # è´ªå¿ƒä¼˜åŒ–å‚æ•°ï¼ˆæ¨èï¼‰
    parser.add_argument('--use_greedy_refinement', action='store_true',
                       help='ä½¿ç”¨è´ªå¿ƒå›¾ä¼˜åŒ–ï¼ˆæ¨èï¼‰')
    parser.add_argument('--greedy_max_modifications', type=int, default=10,
                       help='è´ªå¿ƒä¼˜åŒ–çš„æœ€å¤§ä¿®æ”¹æ¬¡æ•°')
    parser.add_argument('--greedy_min_improvement', type=float, default=0.01,
                       help='è´ªå¿ƒä¼˜åŒ–çš„æœ€å°LLæ”¹è¿›é˜ˆå€¼')
    parser.add_argument('--greedy_eval_epochs', type=int, default=15,
                       help='è´ªå¿ƒè¯„ä¼°æ—¶çš„è®­ç»ƒè½®æ•°ï¼ˆé™ä½ä»¥åŠ é€Ÿï¼‰')
    parser.add_argument('--greedy_max_candidates', type=int, default=30,
                       help='æ¯ç§æ“ä½œæœ€å¤šæµ‹è¯•çš„å€™é€‰æ•°ï¼ˆåŠ é€Ÿå¤§å›¾ï¼‰')
    parser.add_argument('--greedy_start_iter', type=int, default=0,
                       help='ä»ç¬¬å‡ è½®è¿­ä»£å¼€å§‹ä½¿ç”¨è´ªå¿ƒä¼˜åŒ–')
    
    # åŸºçº¿å‚è€ƒå‚æ•°
    parser.add_argument('--use_baseline_reference', action='store_true',
                       help='ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•çš„é¢„æµ‹ç»“æœä½œä¸ºLLMå‚è€ƒ')
    parser.add_argument('--baseline_predict_dir', type=str, default='predict',
                       help='predictç›®å½•è·¯å¾„ï¼ˆåŒ…å«é¢„å…ˆè®¡ç®—çš„é¢„æµ‹ç»“æœï¼‰')
    parser.add_argument('--baseline_methods', type=str, nargs='+', default=['corr', 'invcov'],
                       help='è¦åŠ è½½çš„åŸºçº¿æ–¹æ³•åˆ—è¡¨ï¼Œå¦‚: corr invcov notears')
    parser.add_argument('--baseline_top_k', type=int, default=10,
                       help='æ¯ä¸ªæ–¹æ³•æ˜¾ç¤ºtop-kä¸ªæœ€å¼ºå…³ç³»')
    parser.add_argument('--baseline_threshold', type=float, default=0.5,
                       help='ç­›é€‰é˜ˆå€¼çš„ç™¾åˆ†ä½æ•°ï¼ˆ0-100ï¼‰')
    parser.add_argument('--use_local_amendment', action='store_true',
                       help='ä½¿ç”¨æœ¬åœ°ä¿®æ­£')
    parser.add_argument('--choose_best', action='store_true',
                       help='åœ¨åˆå§‹é˜¶æ®µæ¯”è¾ƒåŸºçº¿æ–¹æ³•å’Œå…¨å±€LLMç”Ÿæˆçš„ç»“æœï¼Œé€‰æ‹©BICæ›´å¥½çš„é‚£ä¸ª')
    parser.add_argument('--use_intervention_test', action='store_true',
                       help='å¯ç”¨å¹²é¢„å®éªŒéªŒè¯é€»è¾‘')
    parser.add_argument('--num_intervention_experiments', type=int, default=3,
                       help='æ¯è½®å…è®¸æå‡ºçš„æœ€å¤§å¹²é¢„å®éªŒæ•°')
    
    args = parser.parse_args()
    
    # æ ¹æ®æ¨¡å‹å’ŒåŸºçº¿æ–¹æ³•æ„å»ºè¾“å‡ºè·¯å¾„
    model_tag = "openai"
    if args.llm_type == 'local' and args.model_path:
        model_tag = os.path.basename(args.model_path.rstrip('/'))
    elif args.llm_type == 'openai':
        model_tag = "openai"
    
    baseline_tag = "no_baseline"
    if args.use_baseline_reference:
        # å¦‚æœ baseline_methods æ˜¯åˆ—è¡¨ï¼Œå°†å…¶æ’åºå¹¶è¿æ¥
        if isinstance(args.baseline_methods, list):
            baseline_tag = "_".join(sorted(args.baseline_methods))
        else:
            baseline_tag = str(args.baseline_methods)
    if args.mode == 'llm-only':
        baseline_tag = "llm-only"
    
    # æ›´æ–° output_dir
    args.output_dir = os.path.join(args.output_dir, f"{model_tag}_{baseline_tag}")

    # ========== æ‰¹é‡å®éªŒæ¨¡å¼ ==========
    if args.mode in ['batch', 'llm-only']:
        print("\n" + "="*80)
        print(f"CMA {'LLM-ONLY' if args.mode == 'llm-only' else 'BATCH'} EXPERIMENTS")
        print("="*80)
        print(f"Configuration:")
        print(f"  CSV Path: {args.csv_path}")
        print(f"  Output Dir: {args.output_dir}")
        print(f"  LLM Type: {args.llm_type}")
        if args.llm_type == 'local':
            print(f"  Model Path: {args.model_path}")
        else:
            print(f"  API URL: {args.openai_url}")
        
        if args.mode == 'llm-only':
            print(f"  Mode: LLM-only (Stop after first successful global graph)")
        else:
            print(f"  Iterations: {args.num_iterations if args.num_iterations else 'Auto (per node)'}")
            if args.num_iterations is None:
                print(f"  Iterations per node: {args.iterations_per_node}")
            print(f"  Early stopping patience: {args.early_stopping_patience}")
        
        print(f"  Trials per dataset: {args.num_runs}")
        print(f"  Epochs: {args.num_epochs}")
        print(f"  Device: {args.device}")
        print(f"  Hill Climbing: {args.use_hill_climbing}")
        if args.use_hill_climbing:
            print(f"  Acceptance Tolerance: {args.acceptance_tolerance}")
        print(f"  Use Skeleton: {args.use_skeleton}")
        if args.use_skeleton:
            print(f"  Skeleton Alpha: {args.skeleton_alpha}")
            print(f"  Skeleton Max Cond Size: {args.skeleton_max_cond_size}")
        print(f"  Use Baseline Reference: {args.use_baseline_reference}")
        if args.use_baseline_reference:
            print(f"  Baseline Predict Dir: {args.baseline_predict_dir}")
            print(f"  Baseline Methods: {args.baseline_methods}")
            print(f"  Baseline Top-K: {args.baseline_top_k}")
            print(f"  Baseline Threshold Percentile: {args.baseline_threshold}")
        print(f"  Choose Best Initial: {args.choose_best}")
        print("="*80 + "\n")
        
        # éªŒè¯æ–‡ä»¶
        if not os.path.exists(args.csv_path):
            print(f"âŒ Error: CSV file not found: {args.csv_path}")
            exit(1)
        
        if args.llm_type == 'local' and not os.path.exists(args.model_path):
            print(f"âŒ Error: Model path not found: {args.model_path}")
            exit(1)
        
        # åˆ›å»ºæ‰¹é‡è¿è¡Œå™¨
        runner = BatchExperimentRunner(
            csv_config_path=args.csv_path,
            base_output_dir=args.output_dir,
            llm_type=args.llm_type,
            llm_model_path=args.model_path if args.llm_type == 'local' else None,
            openai_base_url=args.openai_url if args.llm_type == 'openai' else None,
            openai_api_key=args.openai_key if args.llm_type == 'openai' else None
        )
        
        # è¿è¡Œæ‰¹é‡å®éªŒ
        runner.run_all_experiments(
            split='test',
            num_runs=args.num_runs,
            num_iterations=args.num_iterations,
            iterations_per_node=args.iterations_per_node,
            early_stopping_patience=args.early_stopping_patience,
            num_epochs=args.num_epochs,
            device=args.device,
            learning_rate=0.01,
            temperature=0.6,
            use_hill_climbing=args.use_hill_climbing,
            acceptance_tolerance=args.acceptance_tolerance,
            verbose=args.verbose,
            max_retries=args.max_retries,
            use_skeleton=args.use_skeleton,
            skeleton_alpha=args.skeleton_alpha,
            skeleton_max_cond_size=args.skeleton_max_cond_size,
            use_notears_refinement=args.use_notears_refinement,
            notears_use_mlp=args.notears_use_mlp,
            use_greedy_refinement=args.use_greedy_refinement,
            greedy_max_modifications=args.greedy_max_modifications,
            greedy_min_improvement=args.greedy_min_improvement,
            greedy_eval_epochs=args.greedy_eval_epochs,
            greedy_max_candidates=args.greedy_max_candidates,
            greedy_start_iter=args.greedy_start_iter,
            use_mcts=args.use_mcts,
            mcts_simulations=args.mcts_simulations,
            mcts_exploration_weight=args.mcts_exploration_weight,
            mcts_max_depth=args.mcts_max_depth,
            llm_only=(args.mode == 'llm-only'),
            choose_best=args.choose_best,
            # åŸºçº¿å‚è€ƒå‚æ•°
            use_baseline_reference=args.use_baseline_reference,
            baseline_predict_dir=args.baseline_predict_dir,
            baseline_methods=args.baseline_methods,
            baseline_top_k=args.baseline_top_k,
            baseline_threshold=args.baseline_threshold,
            use_local_amendment=args.use_local_amendment,
            use_intervention_test=args.use_intervention_test,
            num_intervention_experiments=args.num_intervention_experiments
        )
        
        print(f"\nâœ… All experiments completed!")
        print(f"Results saved to: {args.output_dir}")
        print(f"Summary: {args.output_dir}/experiments_summary.json\n")