"""
æœç´¢ç­–ç•¥æ¨¡å— - å°è£…ä¸åŒçš„å›¾æœç´¢æ–¹æ³•
åŒ…å« Hill Climbing å’Œ MCTS
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional
import os
import json
import copy
import math
import random
from src.utils.metrics import _compute_metrics


class SearchStrategy(ABC):
    """æœç´¢ç­–ç•¥åŸºç±»"""
    
    def __init__(self, pipeline):
        self.pipeline = pipeline
        self.iteration_history = []
    
    @abstractmethod
    def search(self, **kwargs) -> Dict:
        """æ‰§è¡Œæœç´¢ï¼Œè¿”å›ç»“æœå­—å…¸"""
        pass


class HillClimbingStrategy(SearchStrategy):
    """çˆ¬å±±æœç´¢ç­–ç•¥ï¼ˆåŒ…å« refinement è°ƒç”¨ï¼‰"""
    
    def __init__(self, pipeline, acceptance_tolerance: float = 0.0):
        super().__init__(pipeline)
        self.acceptance_tolerance = acceptance_tolerance
    
    def search(
        self,
        num_iterations: int = 3,
        early_stopping_patience: int = 3,
        num_epochs: int = 100,
        learning_rate: float = 0.01,
        temperature: float = 0.6,
        llm_model_name: str = "gpt-4o",
        max_tokens: int = 4096,
        verbose: bool = True,
        max_retries: int = 10,
        use_local_amendment: bool = True,
        llm_only: bool = False,
        choose_best: bool = False
    ) -> Dict:
        """Hill Climbing æœç´¢ä¸»å¾ªç¯"""
        
        print(f"\n{'='*70}")
        print(f"SEARCH STRATEGY: Hill Climbing")
        if llm_only:
            print(f"MODE: LLM-only (Stop after first successful global graph)")
        print(f"Acceptance Tolerance: {self.acceptance_tolerance}")
        print(f"Early Stopping Patience: {early_stopping_patience}")
        print(f"Choose Best Initial: {choose_best}")
        print(f"{'='*70}\n")
        
        # åˆå§‹åŒ–
        best_graph = None
        best_ll = float('-inf')
        current_graph = None
        
        t = 0
        error_memory = []
        failed_attempts = []
        consecutive_rejections = 0  # è¿ç»­è¢«æ‹’ç»çš„æ¬¡æ•°
        
        # ä¸»å¾ªç¯
        for i in range(num_iterations):
            print(f"\n{'ğŸ”„ '*35}")
            print(f"ITERATION {t}, Real iteration: {i+1}/{num_iterations}")
            if consecutive_rejections > 0:
                print(f"Consecutive rejections: {consecutive_rejections}/{early_stopping_patience}")
            print(f"{'ğŸ”„ '*35}")
            if error_memory:
                error_summary = "\n".join([f"Attempt {i+1}: {err}" 
                                            for i, err in enumerate(error_memory)])
                memory = f"âš ï¸ Previous Failed Attempts:\n{error_summary}\n\nPlease try different modifications."
            else:
                memory = None
            if current_graph is not None or (self.pipeline.use_baseline_reference==False):
                # ===== æ–°å¢ï¼šå¹²é¢„å®éªŒè¯¢é—®é˜¶æ®µ =====
                exp_reasoning = None
                candidate_operations = None
                if self.pipeline.use_intervention_test:
                    # ä¼ å…¥ edge_notes ä»¥å¼•å¯¼å®éªŒå»ºè®®
                    current_edge_notes = None
                    if current_graph and 'metadata' in current_graph:
                        current_edge_notes = current_graph['metadata'].get('edge_notes', {})
                    
                    experiments, exp_reasoning, candidate_operations = self.pipeline.hypothesis_generator.propose_experiments(
                        variable_list=self.pipeline.variable_list,
                        domain_name=self.pipeline.domain_name,
                        domain_context=self.pipeline.domain_context,
                        previous_graph=current_graph,
                        num_experiments=self.pipeline.num_intervention_experiments,
                        model=llm_model_name,
                        temperature=temperature,
                        edge_notes=current_edge_notes  # å¼•å¯¼æ¢ç´¢
                    )
                    if experiments:
                        new_evidence = self.pipeline.intervention_tester.run_experiments(experiments)
                        if new_evidence:
                            self.pipeline.accumulated_evidence.append(new_evidence)
                            self.pipeline.policy_verifier.update_evidence(new_evidence)
                            self.pipeline.policy_verifier.update_evidence(new_evidence) # æ›´æ–°ç­–ç•¥æ ¡éªŒå™¨
                            print(f"\nğŸ”¬ New Interventional Evidence Obtained:\n{new_evidence}")
                
                # æ±‡æ€»æ‰€æœ‰å†å²è¯æ®
                interventional_evidence = "\n".join(self.pipeline.accumulated_evidence) if self.pipeline.accumulated_evidence else None
                if interventional_evidence and verbose:
                    print(f"  [Debug] Passing {len(self.pipeline.accumulated_evidence)} cumulative evidence reports to LLM")

                # ===== è·å–å†å²æ¨ç†å’Œå› æœæ¡£æ¡ˆ =====
                # ä¼˜å…ˆçº§ï¼šå®éªŒåŠ¨æœºæ¨ç† > ä¹‹å‰çš„ä¿®æ­£æ¨ç†
                previous_reasoning = exp_reasoning
                confirmed_edges = None
                edge_notes = None
                
                if current_graph and 'metadata' in current_graph:
                    if not previous_reasoning:
                        previous_reasoning = current_graph['metadata'].get('reasoning')
                    confirmed_edges = current_graph['metadata'].get('confirmed_edges', [])
                    edge_notes = current_graph['metadata'].get('edge_notes', {})

                # ===== ç”Ÿæˆå‡è®¾ï¼ˆä¼ å…¥è¯æ®å’Œå†å²æ¨ç†ï¼‰=====
                structured_graph, validation_info = self.pipeline.hypothesis_generator.generate_hypothesis(
                    variable_list=self.pipeline.variable_list,
                    domain_name=self.pipeline.domain_name,
                    domain_context=self.pipeline.domain_context,
                    previous_graph=current_graph,
                    memory=memory,
                    iteration=t,
                    model=llm_model_name,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    use_local_amendment=use_local_amendment,
                    num_edge_operations=1,
                    skeleton_constraints=getattr(self.pipeline, 'skeleton_constraints', None),
                    failed_attempts=failed_attempts,
                    baseline_reference=getattr(self.pipeline, 'baseline_reference_text', None),
                    interventional_evidence=interventional_evidence,  # ä¼ å…¥è¯æ®
                    previous_reasoning=previous_reasoning,  # ä¼ å…¥å†å²æ¨ç†
                    confirmed_edges=confirmed_edges,
                    edge_notes=edge_notes,
                    candidate_operations=candidate_operations  # ä¼ å…¥é™å®šçš„å€™é€‰æ“ä½œ
                )
                assert validation_info is not None, "validation_info cannot be None" # TODO:
                assert 'has_cycle' in validation_info, "has_cycle must be in validation_info"
                success_flag = validation_info['success']
                has_cycle_flag = validation_info['has_cycle']
                if has_cycle_flag: assert success_flag is False, "has_cycle must be False if success is False"
                if verbose and not success_flag:
                    print(f"  [Debug] validation_info: success={success_flag}, has_cycle={has_cycle_flag}")
                    if has_cycle_flag and 'cycle_path' in validation_info:
                        print(f"  [Debug] cycle_path: {validation_info['cycle_path']}")

                if (structured_graph is not None and 
                    validation_info is not None and 
                    validation_info.get('success') is True):  # æ˜ç¡®æ£€æŸ¥æ˜¯ Trueï¼Œè€Œä¸æ˜¯é»˜è®¤å€¼
                    
                    # ===== å¼ºåˆ¶ç­–ç•¥æ ¡éªŒ (EVIDENCE-FIRST POLICY) =====
                    proposed_ops = structured_graph['metadata'].get('proposed_operations')
                    
                    if proposed_ops:
                        policy_violations = self.pipeline.policy_verifier.verify_operations(proposed_ops)
                        
                        if policy_violations:
                            print(f"âš ï¸  EVIDENCE-FIRST POLICY VIOLATION DETECTED!")
                            for violation in policy_violations:
                                print(f"   - {violation}")
                            
                            # æ ‡è®°ä¸ºéªŒè¯å¤±è´¥ï¼Œå¹¶å°†è¿åç­–ç•¥çš„ä¿¡æ¯åŠ å…¥é”™è¯¯æ¶ˆæ¯
                            validation_info['success'] = False
                            validation_info['error_messages'].extend(policy_violations)
                            # å¼ºåˆ¶è¿›å…¥å¤±è´¥å¤„ç†æµç¨‹
                        else:
                            pass
                    else:
                        # åˆå§‹ç”Ÿæˆæˆ–å…¨å±€ä¿®æ­£ï¼Œæ²¡æœ‰å…·ä½“æ“ä½œåˆ—è¡¨ï¼Œè·³è¿‡é’ˆå¯¹æ“ä½œçš„æ ¡éªŒ
                        pass

                # æ£€æŸ¥éªŒè¯æˆ–ç­–ç•¥æ ¡éªŒæ˜¯å¦æˆåŠŸ
                if structured_graph is None or validation_info is None or not validation_info.get('success'):
                    # ä»error_messagesæ•°ç»„ä¸­è·å–é”™è¯¯ä¿¡æ¯
                    error_messages = validation_info.get('error_messages', []) if validation_info else ["Unknown validation error"]
                    error_msg = '; '.join(error_messages)
                    print(f"âŒ Validation failed: {error_msg}")
                    error_memory.append(error_msg)
                    
                    # å¦‚æœæœ‰æå‡ºçš„æ“ä½œï¼Œä¹ŸåŠ å…¥ failed_attemptsï¼Œä»¥ä¾¿ä¸‹ä¸€è½®é¿å‘
                    if structured_graph and 'metadata' in structured_graph:
                        proposed_ops = structured_graph['metadata'].get('proposed_operations', [])
                        if proposed_ops:
                            failed_ops = [(op['type'], op['parent'], op['child']) for op in proposed_ops]
                            failed_attempts.extend(failed_ops)
                            print(f"  [Debug] Added {len(failed_ops)} failed operations to failed_attempts")
                    
                    # è®°å½•éªŒè¯å¤±è´¥çš„è¿­ä»£
                    self.iteration_history.append({
                        'iteration': t,
                        'graph': structured_graph if structured_graph is not None else None,
                        'accepted': False,
                        'best_ll': best_ll,
                        'current_ll': float('-inf'),
                        'metrics': {},
                        'results': {
                            'log_likelihood': float('-inf'),
                            'bic': None,
                            'num_edges': 0
                        },
                        'validation_failed': True,
                        'rejection_reason': f"Validation failed: {error_msg}",
                        'validation_info': validation_info
                    })
                    
                    # ä¿å­˜éªŒè¯å¤±è´¥çš„å›¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    if structured_graph is not None: # TODO: currently not used
                        graph_path = os.path.join(self.pipeline.output_dir, f"graph_t{t}_validation_failed.json")
                        with open(graph_path, 'w') as f:
                            json.dump(structured_graph, f, indent=2)
                    
                    t += 1
                    continue
            else:
                # åˆå§‹è¿­ä»£ (t=0) ä¸”å¯ç”¨ baseline_reference æ—¶
                # ä»åŸºçº¿ç»“æœä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªæ–¹æ³•ä½œä¸ºæ¯”è¾ƒå¯¹è±¡
                baseline_graph = next(iter(self.pipeline.baseline_structured_graphs.values()))
                
                if choose_best:
                    print(f"\n[Choose Best] Comparing Baseline vs Global LLM hypothesis...")
                    
                    # åˆå§‹é˜¶æ®µä¹Ÿå¯ä»¥åšå¹²é¢„æµ‹è¯•
                    exp_reasoning = None
                    candidate_operations = None
                    if self.pipeline.use_intervention_test:
                        experiments, exp_reasoning, candidate_operations = self.pipeline.hypothesis_generator.propose_experiments(
                            variable_list=self.pipeline.variable_list,
                            domain_name=self.pipeline.domain_name,
                            domain_context=self.pipeline.domain_context,
                            previous_graph=None,
                            num_experiments=self.pipeline.num_intervention_experiments,
                            model=llm_model_name,
                            temperature=temperature,
                            edge_notes={} # åˆå§‹é˜¶æ®µä¸ºç©º
                        )
                        if experiments:
                            new_evidence = self.pipeline.intervention_tester.run_experiments(experiments)
                            if new_evidence:
                                self.pipeline.accumulated_evidence.append(new_evidence)
                                self.pipeline.policy_verifier.update_evidence(new_evidence) # æ›´æ–°ç­–ç•¥æ ¡éªŒå™¨
                                print(f"\nğŸ”¬ New Interventional Evidence Obtained:\n{new_evidence}")
                    
                    interventional_evidence = "\n".join(self.pipeline.accumulated_evidence) if self.pipeline.accumulated_evidence else None

                    global_graph, validation_info = self.pipeline.hypothesis_generator.generate_hypothesis(
                        variable_list=self.pipeline.variable_list,
                        domain_name=self.pipeline.domain_name,
                        domain_context=self.pipeline.domain_context,
                        previous_graph=None,
                        memory=None,
                        iteration=t,
                        model=llm_model_name,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        use_local_amendment=False,
                        num_edge_operations=1,
                        skeleton_constraints=getattr(self.pipeline, 'skeleton_constraints', None),
                        failed_attempts=failed_attempts,
                        baseline_reference=getattr(self.pipeline, 'baseline_reference_text', None),
                        interventional_evidence=interventional_evidence,
                        previous_reasoning=exp_reasoning,
                        confirmed_edges=[],
                        edge_notes={},
                        candidate_operations=candidate_operations
                    )
                    
                    if validation_info and validation_info.get('success'):
                        # è¯„ä¼°ä¸¤è€…
                        self._evaluate_graph(baseline_graph, num_epochs, learning_rate, verbose=False)
                        self._evaluate_graph(global_graph, num_epochs, learning_rate, verbose=False)
                        
                        baseline_bic = baseline_graph['metadata'].get('bic', float('inf'))
                        global_bic = global_graph['metadata'].get('bic', float('inf'))
                        
                        print(f"  - Baseline BIC: {baseline_bic:.2f}")
                        print(f"  - Global LLM BIC: {global_bic:.2f}")
                        
                        if global_bic < baseline_bic:
                            print(f"  ğŸ† Global LLM wins!")
                            structured_graph = global_graph
                        else:
                            print(f"  ğŸ† Baseline wins!")
                            structured_graph = baseline_graph
                    else:
                        print(f"  âš ï¸ Global LLM generation failed validation, falling back to Baseline.")
                        structured_graph = baseline_graph
                else:
                    structured_graph = baseline_graph
                validation_info = None
            
            # è¯„ä¼°å›¾
            current_ll = self._evaluate_graph(structured_graph, num_epochs, learning_rate, verbose=True)
            
            # è®¡ç®— metrics
            metrics = _compute_metrics(self.pipeline, structured_graph)
            structured_graph['metadata']['evaluation_metrics'] = metrics
            print(f"Evaluation metrics:", metrics)
            
            # åº”ç”¨ refinements (NOTEARS, Greedy)
            if not llm_only:
                structured_graph, current_ll = self._apply_refinements(
                    structured_graph, current_ll, t, num_epochs, learning_rate, verbose
                )
            else:
                print(f"[LLM-only] Skipping refinements (NOTEARS/Greedy)")
            
            # Hill Climbing å†³ç­–
            if current_ll >= best_ll - self.acceptance_tolerance:
                # æ¥å—
                if current_ll > best_ll:
                    best_ll = current_ll
                    best_graph = copy.deepcopy(structured_graph)
                
                current_graph = structured_graph
                error_memory = []
                failed_attempts = []
                consecutive_rejections = 0  # é‡ç½®é‡è¯•è®¡æ•°
                
                print(f"\nâœ… ACCEPTED (Hill Climbing)")
                print(f"  Current LL: {current_ll:.4f}")
                print(f"  Best LL: {best_ll:.4f}")
                
                # è®°å½•å†å² - æˆåŠŸçš„è¿­ä»£
                self.iteration_history.append({
                    'iteration': t,
                    'graph': structured_graph,
                    'accepted': True,
                    'best_ll': best_ll,
                    'current_ll': current_ll,
                    'metrics': metrics,
                    'results': {
                        'log_likelihood': current_ll,
                        'bic': structured_graph['metadata'].get('bic', None),
                        'num_edges': structured_graph['metadata']['num_edges']
                    }
                })
                
                # ä¿å­˜å›¾
                graph_path = os.path.join(self.pipeline.output_dir, f"graph_t{t}.json") # TODO: currently not used
                with open(graph_path, 'w') as f:
                    json.dump(structured_graph, f, indent=2)
                
                # LLM-only æ¨¡å¼ï¼šæ¥å—ç¬¬ä¸€ä¸ªæˆåŠŸçš„å›¾ï¼ˆglobalå›¾ï¼‰åç«‹å³åœæ­¢
                if llm_only:
                    print(f"\nâœ… [LLM-only] First successful graph obtained. Terminating iteration.")
                    return {
                        'best_graph': best_graph,
                        'best_ll': best_ll,
                        'current_graph': current_graph,
                        'history': self.iteration_history
                    }

                t += 1
            else:
                # æ‹’ç»
                consecutive_rejections += 1
                print(f"\nâŒ REJECTED: LL = {current_ll:.4f} < {best_ll - self.acceptance_tolerance:.4f}")
                print(f"Consecutive rejections: {consecutive_rejections}/{early_stopping_patience}")
                
                assert 'proposed_operations' in structured_graph['metadata'], "currently not implemented no proposed operations"
                operation_proposed = [(operation['type'], operation['parent'], operation['child']) for operation in structured_graph['metadata']['proposed_operations']]
                error_msg = f"LL too low: {current_ll:.4f}, proposed operations: {operation_proposed}"
                error_memory.append(error_msg)
                failed_attempts.extend(operation_proposed)
                assert len(operation_proposed) == 1, "currently not implemented multiple proposed operations"
                
                # è®°å½•å†å² - å¤±è´¥çš„è¿­ä»£
                self.iteration_history.append({
                    'iteration': t,
                    'graph': structured_graph,
                    'accepted': False,
                    'best_ll': best_ll,
                    'current_ll': current_ll,
                    'metrics': metrics,
                    'results': {
                        'log_likelihood': current_ll,
                        'bic': structured_graph['metadata'].get('bic', None),
                        'num_edges': structured_graph['metadata']['num_edges']
                    },
                    'rejection_reason': error_msg,
                    'proposed_operations': operation_proposed
                })
                
                # ä¿å­˜è¢«æ‹’ç»çš„å›¾ï¼ˆä½¿ç”¨ä¸åŒçš„å‘½åä»¥ä¾¿åŒºåˆ†ï¼‰
                graph_path = os.path.join(self.pipeline.output_dir, f"graph_t{t}_rejected.json") # TODO: currently not used
                with open(graph_path, 'w') as f:
                    json.dump(structured_graph, f, indent=2)
                
                # æ£€æŸ¥æ—©åœ
                if consecutive_rejections >= early_stopping_patience:
                    print(f"\nğŸ›‘ EARLY STOPPING: {consecutive_rejections} consecutive rejections reached.")
                    break
                
                t += 1
        
        return {
            'best_graph': best_graph,
            'best_ll': best_ll,
            'current_graph': current_graph,
            'history': self.iteration_history
        }
    
    def _generate_graph(self, current_graph, iteration, llm_model_name,
                       temperature, max_tokens, max_retries, error_memory, verbose, use_local_amendment=True, 
                       interventional_evidence=None, previous_reasoning=None, confirmed_edges=None, edge_notes=None):
        """ç”Ÿæˆæˆ–ä¿®æ”¹å›¾ï¼ˆå¸¦é‡è¯•ï¼‰"""
        
        retry_count = 0
        failed_initial_attempts = []
        
        while retry_count < max_retries:
            memory = None
            if error_memory:
                error_summary = "\n".join([f"Attempt {i+1}: {err}" 
                                            for i, err in enumerate(error_memory)])
                memory = f"âš ï¸ Previous Failed Attempts:\n{error_summary}\n\nPlease try different modifications."
            
            # æå–å†å²æ¨ç†å’Œæ¡£æ¡ˆ (å¦‚æœæ²¡æœ‰ä»å¤–éƒ¨ä¼ å…¥)
            if current_graph and 'metadata' in current_graph:
                if previous_reasoning is None:
                    previous_reasoning = current_graph['metadata'].get('reasoning')
                if confirmed_edges is None:
                    confirmed_edges = current_graph['metadata'].get('confirmed_edges', [])
                if edge_notes is None:
                    edge_notes = current_graph['metadata'].get('edge_notes', {})
            
            # ç”Ÿæˆå›¾
            structured_graph, validation_info = self.pipeline.hypothesis_generator.generate_hypothesis(
                variable_list=self.pipeline.variable_list,
                domain_name=self.pipeline.domain_name,
                domain_context=self.pipeline.domain_context,
                previous_graph=current_graph,
                memory=memory,
                iteration=iteration,
                model=llm_model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                use_local_amendment=use_local_amendment,
                num_edge_operations=1,
                skeleton_constraints=getattr(self.pipeline, 'skeleton_constraints', None),
                failed_attempts=failed_initial_attempts if failed_initial_attempts else None,
                baseline_reference=getattr(self.pipeline, 'baseline_reference_text', None),
                interventional_evidence=interventional_evidence,
                previous_reasoning=previous_reasoning,
                confirmed_edges=confirmed_edges,
                edge_notes=edge_notes
            )
            # æ£€æŸ¥éªŒè¯ç»“æœ
            # Check both that graph exists and validation passed
            # è°ƒè¯•ï¼šæ‰“å°validation_infoçš„å†…å®¹
            assert validation_info is not None, "validation_info cannot be None" # TODO:
            assert 'has_cycle' in validation_info, "has_cycle must be in validation_info"
            success_flag = validation_info['success']
            has_cycle_flag = validation_info['has_cycle']
            if has_cycle_flag: assert success_flag is False, "has_cycle must be False if success is False"
            if verbose and not success_flag:
                print(f"  [Debug] validation_info: success={success_flag}, has_cycle={has_cycle_flag}")
                if has_cycle_flag and 'cycle_path' in validation_info:
                    print(f"  [Debug] cycle_path: {validation_info['cycle_path']}")
            
            # åªæœ‰åœ¨æ˜ç¡®æ ‡è®°ä¸ºæˆåŠŸæ—¶æ‰æ¥å—å›¾
            # validation_info å¿…é¡»å­˜åœ¨ï¼Œä¸” success å¿…é¡»æ˜ç¡®ä¸º True
            if (structured_graph is not None and 
                validation_info is not None and 
                validation_info.get('success') is True):  # æ˜ç¡®æ£€æŸ¥æ˜¯ Trueï¼Œè€Œä¸æ˜¯é»˜è®¤å€¼
                return structured_graph
            else:
                # ä»error_messagesæ•°ç»„ä¸­è·å–é”™è¯¯ä¿¡æ¯
                error_messages = validation_info.get('error_messages', []) if validation_info else []
                error_msg = '; '.join(error_messages) if error_messages else ('Graph is None' if structured_graph is None else 'Unknown error')
                print(f"  [Retry {retry_count+1}] Validation failed: {error_msg}")
                
                retry_count += 1
        
        return None
    
    def _evaluate_graph(self, graph, num_epochs, learning_rate, verbose=False):
        """è¯„ä¼°å›¾çš„ log-likelihood"""
        fitting_results = self.pipeline.fitting_engine.fit(
            structured_graph=graph,
            data=self.pipeline.data,
            interventions=self.pipeline.interventions,
            variable_type=self.pipeline.variable_type,  # ä¼ é€’å˜é‡ç±»å‹
            num_epochs=num_epochs,
            learning_rate=learning_rate,
            verbose=verbose,
            seed=42
        )
        graph['metadata']['log_likelihood'] = fitting_results['log_likelihood']
        graph['metadata']['num_parameters'] = fitting_results['num_parameters']
        graph['metadata']['bic'] = fitting_results['bic']  # ä¿å­˜BIC
        return fitting_results['log_likelihood']
    
    def _apply_refinements(self, graph, current_ll, iteration, num_epochs, learning_rate, verbose):
        """åº”ç”¨ NOTEARS å’Œ Greedy refinements"""
        
        # NOTEARS refinement
        if self.pipeline.use_notears_refinement and iteration >= self.pipeline.notears_start_iter:
            graph, current_ll = self._apply_notears(graph, current_ll, num_epochs, learning_rate, verbose)
        
        # Greedy refinement
        if self.pipeline.use_greedy_refinement and iteration >= self.pipeline.greedy_start_iter:
            graph, current_ll = self._apply_greedy(graph, current_ll, num_epochs, learning_rate, verbose)
        
        return graph, current_ll
    
    def _apply_notears(self, graph, original_ll, num_epochs, learning_rate, verbose):
        """åº”ç”¨ NOTEARS refinement"""
        
        print(f"\n{'ğŸ”¬'*35}")
        print(f"NOTEARS REFINEMENT")
        print(f"{'ğŸ”¬'*35}")
        
        try:
            if self.pipeline.notears_use_mlp:
                # MLP ç‰ˆæœ¬
                notears_graph, refinement_info = self.pipeline.notears_refiner.refine_graph(
                    initial_graph=graph,
                    data=self.pipeline.data,
                    variable_names=self.pipeline.variable_list,
                    engine=self.pipeline.fitting_engine
                )
                notears_ll = refinement_info['refined_ll']
            else:
                # Ridge ç‰ˆæœ¬
                notears_graph, refinement_info = self.pipeline.notears_refiner.refine_graph(
                    initial_graph=graph,
                    data=self.pipeline.data,
                    variable_names=self.pipeline.variable_list,
                    locally_only=True
                )
                notears_ll = self._evaluate_graph(notears_graph, num_epochs, learning_rate, verbose=False)
            
            print(f"\nLLM LL: {original_ll:.4f}, NOTEARS LL: {notears_ll:.4f}, Î”: {notears_ll-original_ll:+.4f}")
            
            if notears_ll > original_ll:
                print(f"âœ… NOTEARS improved!")
                from src.core.cma_pipeline import _compute_metrics
                metrics = _compute_metrics(self.pipeline, notears_graph)
                notears_graph['metadata']['evaluation_metrics'] = metrics
                notears_graph['metadata']['notears_refinement'] = refinement_info
                print(f"{'ğŸ”¬'*35}\n")
                return notears_graph, notears_ll
            else:
                print(f"âš ï¸ Keeping LLM graph")
                print(f"{'ğŸ”¬'*35}\n")
                return graph, original_ll
        
        except Exception as e:
            print(f"âŒ NOTEARS failed: {e}")
            print(f"{'ğŸ”¬'*35}\n")
            return graph, original_ll
    
    def _apply_greedy(self, graph, original_ll, num_epochs, learning_rate, verbose):
        """åº”ç”¨ Greedy refinement"""
        
        print(f"\n{'ğŸ¯'*35}")
        print(f"GREEDY REFINEMENT")
        print(f"{'ğŸ¯'*35}")
        
        try:
            skeleton_constraints = getattr(self.pipeline, 'skeleton_constraints', None)
            allowed_edges = skeleton_constraints.get('allowed_edges_set') if skeleton_constraints else None
            
            greedy_graph, greedy_info = self.pipeline.greedy_refiner.refine_graph(
                initial_graph=graph,
                data=self.pipeline.data,
                variable_names=self.pipeline.variable_list,
                model_fitting_engine=self.pipeline.fitting_engine,
                interventions=self.pipeline.interventions,
                variable_type=self.pipeline.variable_type,
                skeleton_constraints=skeleton_constraints,
                verbose=True,
                seed=42
            )
            
            greedy_ll = greedy_info['final_ll']
            
            print(f"\nLLM LL: {original_ll:.4f}, Greedy LL: {greedy_ll:.4f}, Î”: {greedy_ll-original_ll:+.4f}")
            
            if greedy_ll > original_ll:
                print(f"âœ… Greedy improved!")
                from src.core.cma_pipeline import _compute_metrics
                metrics = _compute_metrics(self.pipeline, greedy_graph)
                greedy_graph['metadata']['evaluation_metrics'] = metrics
                greedy_graph['metadata']['greedy_refinement'] = greedy_info
                print(f"{'ğŸ¯'*35}\n")
                return greedy_graph, greedy_ll
            else:
                print(f"âš ï¸ Keeping LLM graph")
                print(f"{'ğŸ¯'*35}\n")
                return graph, original_ll
        
        except Exception as e:
            print(f"âŒ Greedy failed: {e}")
            print(f"{'ğŸ¯'*35}\n")
            return graph, original_ll


# æœªæ¥çš„ MCTS ç­–ç•¥
class MCTSNode: # TODO:
    """MCTS æ ‘èŠ‚ç‚¹"""
    
    def __init__(self, graph: Dict, parent: Optional['MCTSNode'] = None, 
                 ll: float = float('-inf'), iteration: int = 0):
        self.graph = graph  # å½“å‰å›¾ç»“æ„
        self.parent = parent  # çˆ¶èŠ‚ç‚¹
        self.children = []  # å­èŠ‚ç‚¹åˆ—è¡¨
        self.visits = 0  # è®¿é—®æ¬¡æ•°
        self.value = 0.0  # ç´¯è®¡å¥–åŠ±
        self.ll = ll  # å½“å‰å›¾çš„log-likelihood
        self.iteration = iteration  # å¯¹åº”çš„è¿­ä»£æ¬¡æ•°
        self.is_fully_expanded = False  # æ˜¯å¦å·²å®Œå…¨æ‰©å±•
    
    def is_leaf(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¶èŠ‚ç‚¹"""
        return len(self.children) == 0
    
    def best_child(self, exploration_weight: float = 1.414) -> 'MCTSNode':
        """ä½¿ç”¨UCB1é€‰æ‹©æœ€ä½³å­èŠ‚ç‚¹"""
        best_score = float('-inf')
        best_child = None
        
        for child in self.children:
            if child.visits == 0:
                # ä¼˜å…ˆé€‰æ‹©æœªè®¿é—®çš„èŠ‚ç‚¹
                return child
            
            # UCB1å…¬å¼: exploitation + exploration
            exploitation = child.value / child.visits
            exploration = exploration_weight * math.sqrt(
                math.log(self.visits) / child.visits
            )
            ucb_score = exploitation + exploration
            
            if ucb_score > best_score:
                best_score = ucb_score
                best_child = child
        
        return best_child
    
    def most_visited_child(self) -> 'MCTSNode':
        """è¿”å›è®¿é—®æ¬¡æ•°æœ€å¤šçš„å­èŠ‚ç‚¹ï¼ˆæœ€ç»ˆé€‰æ‹©ï¼‰"""
        if not self.children:
            return None
        return max(self.children, key=lambda c: c.visits)


class MCTSStrategy(SearchStrategy):
    """è’™ç‰¹å¡æ´›æ ‘æœç´¢ç­–ç•¥"""
    
    def __init__(self, pipeline, num_simulations: int = 100, 
                 exploration_weight: float = 1.414, max_depth: int = 5):
        super().__init__(pipeline)
        self.num_simulations = num_simulations  # æ¯æ¬¡è¿­ä»£çš„æ¨¡æ‹Ÿæ¬¡æ•°
        self.exploration_weight = exploration_weight  # UCB1æ¢ç´¢æƒé‡
        self.max_depth = max_depth  # æœ€å¤§æœç´¢æ·±åº¦
        self.root = None  # æœç´¢æ ‘æ ¹èŠ‚ç‚¹
    
    def search(
        self,
        num_iterations: int = 3,
        early_stopping_patience: int = 3,  # MCTS æš‚ä¸å¼ºåˆ¶ä½¿ç”¨ï¼Œä½†ä¿æŒæ¥å£ä¸€è‡´
        num_epochs: int = 100,
        learning_rate: float = 0.01,
        temperature: float = 0.6,
        llm_model_name: str = "gpt-4o",
        max_tokens: int = 4096,
        verbose: bool = True,
        max_retries: int = 10,
        use_local_amendment: bool = True,
        llm_only: bool = False,
        choose_best: bool = False
    ) -> Dict:
        """MCTS æœç´¢ä¸»å¾ªç¯"""
        
        print(f"\n{'='*70}")
        print(f"SEARCH STRATEGY: Monte Carlo Tree Search (MCTS)")
        if llm_only:
            print(f"MODE: LLM-only (Stop after first successful global graph)")
        print(f"Simulations per iteration: {self.num_simulations}")
        print(f"Exploration weight: {self.exploration_weight}")
        print(f"Max depth: {self.max_depth}")
        print(f"Choose Best Initial: {choose_best}")
        print(f"{'='*70}\n")
        
        best_graph = None
        best_ll = float('-inf')
        
        # ç”Ÿæˆåˆå§‹å›¾
        print(f"ğŸŒ± Generating initial graph...")
        if choose_best and self.pipeline.use_baseline_reference:
            print(f"\n[Choose Best] Comparing Baseline vs Global LLM initial graph...")
            baseline_graph = next(iter(self.pipeline.baseline_structured_graphs.values()))
            global_graph = self._generate_initial_graph(
                llm_model_name, temperature, max_tokens, 
                max_retries, num_epochs, learning_rate, verbose
            )
            
            if global_graph is not None:
                # è¯„ä¼°ä¸¤è€…
                self._evaluate_graph(baseline_graph, num_epochs, learning_rate, verbose=False)
                self._evaluate_graph(global_graph, num_epochs, learning_rate, verbose=False)
                
                baseline_bic = baseline_graph['metadata'].get('bic', float('inf'))
                global_bic = global_graph['metadata'].get('bic', float('inf'))
                
                print(f"  - Baseline BIC: {baseline_bic:.2f}")
                print(f"  - Global LLM BIC: {global_bic:.2f}")
                
                if global_bic < baseline_bic:
                    print(f"  ğŸ† Global LLM wins!")
                    initial_graph = global_graph
                else:
                    print(f"  ğŸ† Baseline wins!")
                    initial_graph = baseline_graph
            else:
                print(f"  âš ï¸ Global LLM generation failed, falling back to Baseline.")
                initial_graph = baseline_graph
        else:
            initial_graph = self._generate_initial_graph(
                llm_model_name, temperature, max_tokens, 
                max_retries, num_epochs, learning_rate, verbose
            )
        
        if initial_graph is None:
            print("âŒ Failed to generate initial graph")
            return {
                'best_graph': None,
                'best_ll': float('-inf'),
                'current_graph': None,
                'history': self.iteration_history
            }
        
        # è¯„ä¼°åˆå§‹å›¾
        initial_ll = self._evaluate_graph(initial_graph, num_epochs, learning_rate, verbose)
        best_graph = copy.deepcopy(initial_graph)
        best_ll = initial_ll
        
        # è®°å½•åˆå§‹å›¾
        from src.core.cma_pipeline import _compute_metrics
        initial_metrics = _compute_metrics(self.pipeline, initial_graph)
        initial_graph['metadata']['evaluation_metrics'] = initial_metrics
        
        self.iteration_history.append({
            'iteration': 0,
            'graph': initial_graph,
            'accepted': True,
            'best_ll': best_ll,
            'current_ll': initial_ll,
            'metrics': initial_metrics,
            'results': {
                'log_likelihood': initial_ll,
                'bic': initial_graph['metadata'].get('bic', None),
                'num_edges': initial_graph['metadata']['num_edges']
            }
        })
        
        # self._save_graph(initial_graph, 0) # TODO: currently not used
        
        if self.pipeline.dataset is not None:
            self.pipeline._evaluate_against_ground_truth(initial_graph)
        
        # LLM-only æ¨¡å¼ï¼šè·å¾—ç¬¬ä¸€ä¸ªæˆåŠŸçš„å›¾åç«‹å³åœæ­¢
        if llm_only:
            print(f"\nâœ… [LLM-only] Initial successful graph obtained. Terminating MCTS.")
            return {
                'best_graph': best_graph,
                'best_ll': best_ll,
                'current_graph': best_graph,
                'history': self.iteration_history
            }

        # åˆå§‹åŒ–MCTSæ ¹èŠ‚ç‚¹
        self.root = MCTSNode(initial_graph, parent=None, ll=initial_ll, iteration=0)
        self.root.visits = 1
        self.root.value = initial_ll
        
        print(f"âœ… Initial graph: LL = {initial_ll:.4f}\n")
        
        # MCTSè¿­ä»£
        for t in range(1, num_iterations):
            print(f"\nğŸ”„ MCTS ITERATION {t}/{num_iterations-1}")
            print(f"Current best LL: {best_ll:.4f}")
            
            # æ‰§è¡Œå¤šæ¬¡MCTSæ¨¡æ‹Ÿ
            for sim in range(self.num_simulations):
                if verbose and (sim + 1) % 10 == 0:
                    print(f"  Simulation {sim + 1}/{self.num_simulations}...")
                
                # MCTSå››ä¸ªæ­¥éª¤
                leaf = self._select(self.root)  # é€‰æ‹©
                child = self._expand(leaf, t, llm_model_name, temperature, 
                                    max_tokens, max_retries, num_epochs, 
                                    learning_rate, verbose)  # æ‰©å±•
                
                if child is not None:
                    reward = self._simulate(child)  # æ¨¡æ‹Ÿï¼ˆè¯„ä¼°ï¼‰
                    self._backpropagate(child, reward)  # å›æº¯
            
            # é€‰æ‹©æœ€ä½³å­èŠ‚ç‚¹ä½œä¸ºä¸‹ä¸€ä¸ªæ ¹èŠ‚ç‚¹
            if self.root.children:
                best_child = self.root.most_visited_child()
                
                if best_child and best_child.ll > best_ll:
                    best_ll = best_child.ll
                    best_graph = copy.deepcopy(best_child.graph)
                    print(f"\nâœ… NEW BEST: LL = {best_ll:.4f} (from {best_child.visits} visits)")
                
                # æ›´æ–°æ ¹èŠ‚ç‚¹åˆ°æœ€ä½³å­èŠ‚ç‚¹
                self.root = best_child
                self.root.parent = None
                
                # è®°å½•åˆ°å†å²
                metrics = _compute_metrics(self.pipeline, best_child.graph)
                best_child.graph['metadata']['evaluation_metrics'] = metrics
                
                self.iteration_history.append({
                    'iteration': t,
                    'graph': best_child.graph,
                    'accepted': True,
                    'best_ll': best_ll,
                    'current_ll': best_child.ll,
                    'metrics': metrics,
                    'results': {
                        'log_likelihood': best_child.ll,
                        'bic': best_child.graph['metadata'].get('bic', None),
                        'num_edges': best_child.graph['metadata']['num_edges']
                    }
                })
                
                # self._save_graph(best_child.graph, t) # TODO: currently not used
                
                if self.pipeline.dataset is not None:
                    self.pipeline._evaluate_against_ground_truth(best_child.graph)
            else:
                print(f"\nâš ï¸ No valid children found, stopping MCTS")
                break
        
        return {
            'best_graph': best_graph,
            'best_ll': best_ll,
            'current_graph': self.root.graph if self.root else best_graph,
            'history': self.iteration_history
        }
    
    def _select(self, node: MCTSNode) -> MCTSNode:
        """é€‰æ‹©é˜¶æ®µï¼šä»æ ¹èŠ‚ç‚¹é€‰æ‹©åˆ°å¶èŠ‚ç‚¹"""
        current = node
        depth = 0
        
        while not current.is_leaf() and depth < self.max_depth:
            current = current.best_child(self.exploration_weight)
            depth += 1
        
        return current
    
    def _expand(self, node: MCTSNode, iteration: int, llm_model_name: str,
                temperature: float, max_tokens: int, max_retries: int,
                num_epochs: int, learning_rate: float, verbose: bool) -> Optional[MCTSNode]:
        """æ‰©å±•é˜¶æ®µï¼šä¸ºå¶èŠ‚ç‚¹ç”Ÿæˆæ–°çš„å­èŠ‚ç‚¹"""
        
        if node.is_fully_expanded:
            return None
        
        # ===== æ–°å¢ï¼šå¹²é¢„å®éªŒè¯¢é—®é˜¶æ®µ =====
        all_evidence = None
        candidate_operations = None
        if self.pipeline.use_intervention_test:
            current_edge_notes = node.graph['metadata'].get('edge_notes', {}) if node.graph and 'metadata' in node.graph else {}
            experiments, _, candidate_operations = self.pipeline.hypothesis_generator.propose_experiments(
                variable_list=self.pipeline.variable_list,
                domain_name=self.pipeline.domain_name,
                domain_context=self.pipeline.domain_context,
                previous_graph=node.graph,
                num_experiments=self.pipeline.num_intervention_experiments,
                model=llm_model_name,
                temperature=temperature,
                edge_notes=current_edge_notes
            )
            if experiments:
                new_evidence = self.pipeline.intervention_tester.run_experiments(experiments)
                if new_evidence:
                    self.pipeline.accumulated_evidence.append(new_evidence)
                    self.pipeline.policy_verifier.update_evidence(new_evidence)
        
        all_evidence = "\n".join(self.pipeline.accumulated_evidence) if self.pipeline.accumulated_evidence else None

        # ä½¿ç”¨LLMç”Ÿæˆå±€éƒ¨ä¿®æ”¹
        modified_graph, validation_info = self.pipeline.hypothesis_generator.generate_hypothesis(
            variable_list=self.pipeline.variable_list,
            domain_name=self.pipeline.domain_name,
            domain_context=self.pipeline.domain_context,
            previous_graph=node.graph,
            memory=None,
            iteration=iteration,
            model=llm_model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            use_local_amendment=True,
            skeleton_constraints=getattr(self.pipeline, 'skeleton_constraints', None),
            interventional_evidence=all_evidence,
            candidate_operations=candidate_operations
        )

        # Check that graph exists and validation passed
        if modified_graph is None or not (validation_info and validation_info.get('success', True)):
            node.is_fully_expanded = True
            return None
        
        # ===== å¼ºåˆ¶ç­–ç•¥æ ¡éªŒ (EVIDENCE-FIRST POLICY) =====
        proposed_ops = modified_graph['metadata'].get('proposed_operations')
        
        if proposed_ops:
            policy_violations = self.pipeline.policy_verifier.verify_operations(proposed_ops)
            
            if policy_violations:
                print(f"âš ï¸  EVIDENCE-FIRST POLICY VIOLATION DETECTED in MCTS expansion!")
                for violation in policy_violations:
                    print(f"   - {violation}")
                # MCTS ä¸­ï¼Œå¦‚æœè¿åç­–ç•¥ï¼Œæˆ‘ä»¬è®¤ä¸ºè¯¥æ‰©å±•æ— æ•ˆ
                return None
        else:
            # åˆå§‹ç”Ÿæˆæˆ–å…¨å±€ä¿®æ­£ï¼Œè·³è¿‡é’ˆå¯¹æ“ä½œçš„æ ¡éªŒ
            pass
        
        # è¯„ä¼°æ–°å›¾
        ll = self._evaluate_graph(modified_graph, num_epochs, learning_rate, verbose=False)
        
        # åº”ç”¨refinementï¼ˆå¦‚æœå¯ç”¨ï¼‰
        modified_graph, ll = self._apply_refinements(
            modified_graph, ll, iteration, num_epochs, learning_rate, verbose=False
        )
        
        # åˆ›å»ºå­èŠ‚ç‚¹
        child = MCTSNode(modified_graph, parent=node, ll=ll, iteration=iteration)
        node.children.append(child)
        
        return child
    
    def _simulate(self, node: MCTSNode) -> float:
        """æ¨¡æ‹Ÿé˜¶æ®µï¼šè¯„ä¼°èŠ‚ç‚¹çš„ä»·å€¼ï¼ˆç›´æ¥è¿”å›LLï¼‰"""
        # å¯¹äºå› æœå›¾æœç´¢ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨LLä½œä¸ºå¥–åŠ±
        # ä¸éœ€è¦éšæœºrolloutï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†ç¡®å®šçš„è¯„ä¼°æŒ‡æ ‡
        return node.ll
    
    def _backpropagate(self, node: MCTSNode, reward: float):
        """å›æº¯é˜¶æ®µï¼šæ›´æ–°è·¯å¾„ä¸Šæ‰€æœ‰èŠ‚ç‚¹çš„ç»Ÿè®¡ä¿¡æ¯"""
        current = node
        
        while current is not None:
            current.visits += 1
            current.value += reward
            current = current.parent
    
    def _generate_initial_graph(self, llm_model_name: str, temperature: float,
                                max_tokens: int, max_retries: int, num_epochs: int,
                                learning_rate: float, verbose: bool) -> Optional[Dict]:
        """ç”Ÿæˆåˆå§‹å›¾ï¼ˆå¸¦é‡è¯•ï¼‰"""
        
        failed_attempts = []
        
        # åˆå§‹å¹²é¢„æµ‹è¯•
        all_evidence = None
        candidate_operations = None
        if self.pipeline.use_intervention_test:
            experiments, _, candidate_operations = self.pipeline.hypothesis_generator.propose_experiments(
                variable_list=self.pipeline.variable_list,
                domain_name=self.pipeline.domain_name,
                domain_context=self.pipeline.domain_context,
                previous_graph=None,
                num_experiments=self.pipeline.num_intervention_experiments,
                model=llm_model_name,
                temperature=temperature,
                edge_notes={}
            )
            if experiments:
                new_evidence = self.pipeline.intervention_tester.run_experiments(experiments)
                if new_evidence:
                    self.pipeline.accumulated_evidence.append(new_evidence)
                    self.pipeline.policy_verifier.update_evidence(new_evidence)
        
        all_evidence = "\n".join(self.pipeline.accumulated_evidence) if self.pipeline.accumulated_evidence else None
        
        for retry in range(max_retries):
            try:
                structured_graph, validation_info = self.pipeline.hypothesis_generator.generate_hypothesis(
                    variable_list=self.pipeline.variable_list,
                    domain_name=self.pipeline.domain_name,
                    domain_context=self.pipeline.domain_context,
                    previous_graph=None,
                    memory=None,
                    iteration=0,
                    model=llm_model_name,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    use_local_amendment=False,
                    skeleton_constraints=getattr(self.pipeline, 'skeleton_constraints', None),
                    failed_attempts=failed_attempts if failed_attempts else None,
                    baseline_reference=getattr(self.pipeline, 'baseline_reference_text', None),
                    interventional_evidence=all_evidence,
                    candidate_operations=candidate_operations
                )
                
                # Check both that graph exists and validation passed
                if structured_graph is not None and validation_info and validation_info.get('success', True):
                    return structured_graph
                else:
                    error_messages = validation_info.get('error_messages', []) if validation_info else []
                    error_msg = '; '.join(error_messages) if error_messages else ('Graph is None' if structured_graph is None else 'Unknown error')
                    print(f"  [Retry {retry+1}] Validation failed: {error_msg}")
                    
                    if structured_graph:
                        failed_attempts.append({
                            'graph': structured_graph,
                            'error': error_msg,
                            'cycle_path': validation_info.get('cycle_path')
                        })
                        
            except (TypeError, AttributeError) as e:
                # å‚æ•°é”™è¯¯æˆ–å±æ€§é”™è¯¯ï¼Œç«‹å³æŠ›å‡ºï¼Œä¸é‡è¯•
                print(f"\nâŒ FATAL ERROR: {type(e).__name__}: {str(e)}")
                print("This is a programming error, not a retry-able failure.")
                raise
            except Exception as e:
                # å…¶ä»–é”™è¯¯ï¼Œå¯ä»¥é‡è¯•
                print(f"  [Retry {retry+1}] Exception: {str(e)}")
                if retry == max_retries - 1:
                    # æœ€åä¸€æ¬¡é‡è¯•ä¹Ÿå¤±è´¥äº†ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    print(f"\nâŒ All {max_retries} retries failed.")
                    raise
        
        return None
    
    def _evaluate_graph(self, graph: Dict, num_epochs: int, 
                       learning_rate: float, verbose: bool = False) -> float:
        """è¯„ä¼°å›¾çš„log-likelihood"""
        from model_fitting import ModelFittingEngine
        
        engine = ModelFittingEngine(device=self.pipeline.device)
        
        results = engine.fit(
            structured_graph=graph,
            data=self.pipeline.data,
            interventions=self.pipeline.interventions,
            variable_type=self.pipeline.variable_type,  # ä¼ é€’å˜é‡ç±»å‹
            num_epochs=num_epochs,
            learning_rate=learning_rate,
            verbose=verbose
        )
        
        return results['log_likelihood']
    
    def _apply_refinements(self, graph: Dict, ll: float, iteration: int,
                          num_epochs: int, learning_rate: float, 
                          verbose: bool) -> tuple:
        """åº”ç”¨NOTEARSå’ŒGreedy refinementï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        
        # NOTEARS refinement
        if (self.pipeline.use_notears_refinement and 
            iteration >= self.pipeline.notears_start_iter):
            
            if verbose:
                print(f"\nğŸ”§ Applying NOTEARS refinement...")
            
            refined_graph = self.pipeline.notears_refiner.refine_graph(
                initial_graph=graph,
                data=self.pipeline.data,
                num_epochs=num_epochs,
                lr=learning_rate
            )
            
            if refined_graph is not None:
                refined_ll = self._evaluate_graph(
                    refined_graph, num_epochs, learning_rate, verbose=False
                )
                
                if refined_ll > ll:
                    if verbose:
                        print(f"  âœ… NOTEARS improved LL: {ll:.4f} â†’ {refined_ll:.4f}")
                    graph = refined_graph
                    ll = refined_ll
                else:
                    if verbose:
                        print(f"  âš ï¸ NOTEARS did not improve (kept original)")
        
        # Greedy refinement
        if (self.pipeline.use_greedy_refinement and 
            iteration >= self.pipeline.greedy_start_iter):
            
            if verbose:
                print(f"\nğŸ”§ Applying Greedy refinement...")
            
            refined_graph = self.pipeline.greedy_refiner.refine_graph(
                initial_graph=graph
            )
            
            if refined_graph is not None:
                refined_ll = self._evaluate_graph(
                    refined_graph, num_epochs, learning_rate, verbose=False
                )
                
                if refined_ll > ll:
                    if verbose:
                        print(f"  âœ… Greedy improved LL: {ll:.4f} â†’ {refined_ll:.4f}")
                    graph = refined_graph
                    ll = refined_ll
                else:
                    if verbose:
                        print(f"  âš ï¸ Greedy did not improve (kept original)")
        
        return graph, ll
    
    def _save_graph(self, graph: Dict, iteration: int):
        """ä¿å­˜å›¾åˆ°JSONæ–‡ä»¶"""
        graph_path = os.path.join(self.pipeline.output_dir, f"graph_t{iteration}.json")
        with open(graph_path, 'w') as f:
            json.dump(graph, f, indent=2)

