"""
ä¸»è¿è¡Œæµç¨‹
CMA Pipeline -CMAå®Œæ•´æµç¨‹ç®¡ç†å™¨
"""

import json
import numpy as np
import os
from typing import Dict, List, Optional

from core.llm_hypothesis import LLMHypothesisGenerator
from reflection.post_processing import PostProcessor
from data_loader import DOMAIN_CONTEXTS
from utils.score_functions import score_graph_with_bic

from utils import ConfigManager, visualize_causal_graph
from utils.metrics import compute_metrics
from llm_loader import LLMLoader, LLMLoaderFactory
from schemas import StructuredGraph, CausalDataset
from searcher import SearchStrategy, SearcherFactory

os.environ['VLLM_ATTENTION_BACKEND'] = 'FLASH_ATTN'

class CMAPipeline:
    def __init__(
        self,
        dataset: CausalDataset
    ):
        """
        åˆå§‹åŒ–CMAæµç¨‹
        
        Args:
            dataset: CausalDatasetå¯¹è±¡
        """
        # åŠ è½½é…ç½®ç®¡ç†å™¨
        self.config = ConfigManager()
        config = self.config

        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = config.get("experiment.output.dir", "./cma_output")
        os.makedirs(self.output_dir, exist_ok=True)

        # è®¾ç½®LLMç±»åž‹
        self.llm_type = config.get("llm.type")

        # åŠ è½½æ•°æ®é›†
        self.dataset = dataset
        self.domain_name = dataset.domain_name
        self.variable_list = dataset.variable_names
        self.data = dataset.data
        self.domain_context = DOMAIN_CONTEXTS.get(dataset.domain_name, "")

        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        if self.dataset:
            self._save_dataset_info()

        # ===== é›†ä¸­åŠ è½½LLM =====
        llm_type = config.get("llm.type")
        print("\n" + "="*70)
        print("INITIALIZING LLM BACKEND")
        print("="*70)
        print(f"LLM Type: {llm_type}")
        self.llm_loader: LLMLoader = LLMLoaderFactory.create_llm_loader(llm_type)
        self.llm_loader.load_model()

        # ç»Ÿä¸€æ³¨å…¥ llm_loader åˆ° hypothesis generator å’Œ post processor
        self.hypothesis_generator = LLMHypothesisGenerator(llm_loader=self.llm_loader)
        # self.post_processor = PostProcessor(llm_loader=self.llm_loader)

        # å­˜å‚¨åŽ†å²
        self.iteration_history = []

        print("âœ“ Pipeline initialized successfully!")
        print("="*70 + "\n")

    def _save_dataset_info(self):
        """ä¿å­˜æ•°æ®é›†ä¿¡æ¯"""
        assert self.dataset is not None, "Dataset must be provided to save dataset info."
        info = {
            "domain": self.dataset.domain_name,
            "n_variables": self.dataset.n_variables,
            "n_samples_total": self.dataset.n_samples,
            "n_samples_used": len(self.data),
            "variable_names": self.dataset.variable_names,
            "ground_truth_edges": self.dataset.get_ground_truth_edges(),
        }

        info_path = os.path.join(self.output_dir, f"{self.domain_name}_dataset_info.json")
        with open(info_path, 'w') as f:
            json.dump(info, f, indent=2)
        print(f"[Info] Dataset info saved to {info_path}")

    def run(
        self,
        verbose: bool = True
    ) -> None:
        """è¿è¡Œå®Œæ•´çš„CMAæµç¨‹"""
        num_iterations = self.config.get("training.num_iterations")

        print("\n" + "="*70)
        print(f"STARTING CMA PIPELINE: {self.domain_name.upper()}")
        print("="*70)

        # æ‰“å°æ•°æ®é›†æ‘˜è¦
        self.dataset.print_summary()

        print(f"LLM Type: {self.llm_type}")
        print(f"Iterations: {num_iterations}")
        print(f"Output directory: {self.output_dir}")
        print("="*70 + "\n")

        previous_graph = None
        previous_results = None
        memory = None
        best = None

        # 1. ç”Ÿæˆåˆå§‹å›¾ï¼Œå¹¶åˆ›å»ºæœç´¢ç­–ç•¥
        print("ðŸ”„ " * 35)
        print("GENERATING INITIAL HYPOTHESIS GRAPH & SEARCH STRATEGY")
        print("ðŸ”„ " * 35)
        strategy_name = self.config.get("strategy", "linear")

        initial_graph = self.hypothesis_generator.generate_initial_hypothesis(
            variable_list=self.variable_list,
            domain_name=self.domain_name,
            domain_context=self.domain_context
        )
        if initial_graph is None:
            print("Error: Failed to generate initial hypothesis graph.")
            return
        visualize_causal_graph(initial_graph)

        self.searcher: SearchStrategy = SearcherFactory.create_searcher(
            strategy_name=strategy_name,
            initial_graph=initial_graph
        )

        print(f"Initial hypothesis graph generated. {strategy_name} search strategy initialized.")

        # 2. å¾ªçŽ¯ï¼š
        #   èŽ·å–éœ€è¦ä¿®æ”¹å›¾ï¼ˆç”±searcheræä¾›ï¼‰ï¼›
        #   å¦‚æžœ metadata ä¸­ is_final_graph ä¸º Trueï¼Œåˆ™ continue;
        #   ä½¿ç”¨ hypothesis_generator ç”Ÿæˆæ–°å‡è®¾å›¾ï¼ˆåˆ—è¡¨ï¼‰ï¼›
        #   ä½¿ç”¨ score_functions è¯„åˆ†æ–°å‡è®¾å›¾ï¼›
        #   å°†è¯„åˆ†ä¸Šå‡çš„å›¾åŠ å…¥ searcher;
        for t in range(1, num_iterations+1):
            print("\n" + "ðŸ”„ "*35)
            print(f"ITERATION {t}")
            print("ðŸ”„ "*35)

            # èŽ·å–éœ€è¦ä¿®æ”¹çš„å›¾
            current_graph = self.searcher.search()
            if current_graph.metadata.is_final_graph:
                print(f"Iteration {t}: Graph marked as final by LLM. Skipping modification.")
                continue

            # ç”Ÿæˆæ–°å‡è®¾å›¾
            new_graph = self.hypothesis_generator.generate_next_hypothesis(
                variable_list=self.variable_list,
                domain_name=self.domain_name,
                domain_context=self.domain_context,
                previous_graph=current_graph,
                memory=memory,
                iteration=t,
                num_edge_operations=self.config.get("training.num_edge_operations")
            )

            if new_graph is None:
                print(f"Iteration {t}: No new hypothesis generated.")
                continue

            visualize_causal_graph(new_graph)

            # è¯„åˆ†æ–°å›¾
            fitting_results = score_graph_with_bic(
                structured_graph=new_graph,
                data=self.data,
                variable_names=self.variable_list
            )

            # å°†è¯„åˆ†ç»“æžœæ·»åŠ åˆ°å›¾çš„å…ƒæ•°æ®
            new_graph.metadata.log_likelihood = fitting_results['cv_log_likelihood']
            new_graph.metadata.bic = fitting_results['bic']
            new_graph.metadata.num_parameters = fitting_results['num_parameters']

            # å°†æ–°å›¾å’Œè¯„åˆ†ç»“æžœåŠ å…¥æœç´¢å™¨
            self.searcher.update([new_graph])

        # ===== 3. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š =====
        final_report = self._generate_final_report()

        report_path = os.path.join(self.output_dir, "final_report.txt")
        with open(report_path, 'w') as f:
            f.write(final_report)

        print("\n" + "="*70)
        print("CMA PIPELINE COMPLETED")
        print("="*70)
        print(final_report)
        print("="*70 + "\n")

        # for i in range(num_iterations):
        #     print("\n" + "ðŸ”„ "*35)
        #     print(f"ITERATION {t}")
        #     print("ðŸ”„ "*35)

        #     # ===== æ­¥éª¤1: å‡è®¾ç”Ÿæˆ  =====
        #     structured_graph = self.hypothesis_generator.generate_hypothesis(
        #         variable_list=self.variable_list,
        #         domain_name=self.domain_name,
        #         domain_context=self.domain_context,
        #         previous_graph=previous_graph,
        #         memory=memory,
        #         iteration=t,
        #         num_edge_operations=3
        #     )
        #     if structured_graph is None:
        #         continue

        #     if verbose:
        #         self.hypothesis_generator.visualize_graph(structured_graph)

        #     # ä¿å­˜å‡è®¾ï¼ˆä½¿ç”¨ Pydantic çš„ model_dump è½¬ä¸ºå­—å…¸å†åºåˆ—åŒ–ï¼‰
        #     graph_path = os.path.join(self.output_dir, f"graph_t{t}.json")
        #     with open(graph_path, 'w') as f:
        #         json.dump(structured_graph.model_dump(mode='python'), f, indent=2)

        #     # ===== æ­¥éª¤2: ä½¿ç”¨æ ‡å‡† BIC è¯„åˆ† =====
        #     fitting_results = score_graph_with_bic(
        #         structured_graph=structured_graph,
        #         data=self.data,
        #         variable_names=self.variable_list
        #     )

        #     # å°†è¯„åˆ†ç»“æžœæ·»åŠ åˆ°å›¾çš„å…ƒæ•°æ®
        #     structured_graph.metadata.log_likelihood = fitting_results['cv_log_likelihood']
        #     structured_graph.metadata.bic = fitting_results['bic']
        #     structured_graph.metadata.num_parameters = fitting_results['num_parameters']

        #     # ===== è®°å½•åŽ†å² =====
        #     self.iteration_history.append({
        #         'iteration': t,
        #         'graph': structured_graph,
        #         'results': fitting_results,
        #         # 'memory': memory,
        #         'metrics':compute_metrics(self, structured_graph)
        #     })

        #     # ===== æ›´æ–°å‰ä¸€è½®çš„ä¿¡æ¯ =====
        #     previous_graph = structured_graph
        #     previous_results = fitting_results

        #     # ===== è¯„ä¼°ä¸ŽçœŸå®žå›¾çš„å·®è·(å¦‚æžœæœ‰) =====
        #     if self.dataset and verbose:
        #         self._evaluate_against_ground_truth(structured_graph)

        #     # ===== æå‰ç»ˆæ­¢æ£€æŸ¥ =====
        #     if t > 0:
        #         ll_change = (fitting_results['cv_log_likelihood'] -
        #                    self.iteration_history[t-1]['results']['cv_log_likelihood'])

        #         if abs(ll_change) < 0.01:
        #             print(f"\nâš ï¸  Convergence detected (Î”LL={ll_change:.4f}). Stopping early.")
        #             break
        #     t += 1

        # # ===== ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š =====
        # final_report = self._generate_final_report()

        # report_path = os.path.join(self.output_dir, "final_report.txt")
        # with open(report_path, 'w') as f:
        #     f.write(final_report)

        # print("\n" + "="*70)
        # print("CMA PIPELINE COMPLETED")
        # print("="*70)
        # print(final_report)
        # print("="*70 + "\n")

        # return self.iteration_history[-1] if len(self.iteration_history) > 0 else None

    def _evaluate_against_ground_truth(self, predicted_graph: StructuredGraph):
        """è¯„ä¼°é¢„æµ‹å›¾ä¸ŽçœŸå®žå›¾çš„å·®è·"""

        assert self.dataset is not None, "Dataset must be provided to evaluate against ground truth"

        # æå–é¢„æµ‹çš„è¾¹
        predicted_edges = set()
        for node in predicted_graph.nodes:
            child = node.name
            for parent in node.parents:
                parent_idx = self.variable_list.index(parent)
                child_idx = self.variable_list.index(child)
                predicted_edges.add((parent_idx, child_idx))

        # æå–çœŸå®žçš„è¾¹
        true_edges = set()
        for i in range(self.dataset.n_variables):
            for j in range(self.dataset.n_variables):
                if self.dataset.ground_truth_graph[i, j] == 1:
                    true_edges.add((i, j))

        # è®¡ç®—æŒ‡æ ‡
        true_positive = len(predicted_edges & true_edges)
        false_positive = len(predicted_edges - true_edges)
        false_negative = len(true_edges - predicted_edges)

        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        print("\n" + "-"*70)
        print("EVALUATION AGAINST GROUND TRUTH:")
        print("-"*70)
        print(f"True Positive (correct edges): {true_positive}")
        print(f"False Positive (incorrect edges): {false_positive}")
        print(f"False Negative (missing edges): {false_negative}")
        print(f"Precision: {precision:.3f}")
        print(f"Recall: {recall:.3f}")
        print(f"F1 Score: {f1:.3f}")

        if false_positive > 0:
            print(f"\nIncorrect edges added:")
            for parent_idx, child_idx in (predicted_edges - true_edges):
                print(f"  {self.variable_list[parent_idx]} â†’ {self.variable_list[child_idx]}")

        if false_negative > 0:
            print(f"\nMissing edges:")
            for parent_idx, child_idx in (true_edges - predicted_edges):
                print(f"  {self.variable_list[parent_idx]} â†’ {self.variable_list[child_idx]}")

        print("-"*70)

    def _generate_final_report(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""

        lines = [
            f"CMA Final Report: {self.domain_name}",
            "="*70,
        ]

        # æ•°æ®é›†ä¿¡æ¯
        if self.dataset:
            lines.extend([
                f"\nDataset Information:",
                f"  Variables: {self.dataset.n_variables}",
                f"  Samples used: {len(self.data)}",
                f"  Ground truth edges: {self.dataset.ground_truth_graph.sum()}",
            ])
        else:
            lines.extend([
                f"\nVariables: {len(self.variable_list)}",
                f"Data samples: {self.data.shape[0]}",
            ])

        lines.extend([
            f"\nTotal iterations: {len(self.iteration_history)}",
            "\n" + "-"*70,
            "Iteration Summary:",
            "-"*70
        ])

        for record in self.iteration_history:
            t = record['iteration']
            ll = record['results']['cv_log_likelihood']
            graph: StructuredGraph = record['graph']
            edges = graph.metadata.num_edges
            lines.append(f"  t={t}: LL={ll:.4f}, Edges={edges}")

        # æœ€ä½³è¿­ä»£
        if len(self.iteration_history) > 0:
            best_idx = max(range(len(self.iteration_history)), 
                        key=lambda i: self.iteration_history[i]['results']['cv_log_likelihood'])
            best_ll = self.iteration_history[best_idx]['results']['cv_log_likelihood']

            lines.extend([
                "\n" + "-"*70,
                f"Best iteration: t={best_idx} (LL={best_ll:.4f})",
                "-"*70,
                "\nFinal Causal Structure:"
            ])

            final_graph: StructuredGraph = self.iteration_history[-1]['graph']
            for node in final_graph.nodes:
                parents = node.parents
                if parents:
                    for parent in parents:
                        lines.append(f"  {parent} â†’ {node.name}")
                else:
                    lines.append(f"  {node.name} (root)")

        # å¦‚æžœæœ‰çœŸå®žå›¾,æ·»åŠ å¯¹æ¯”
        if self.dataset:
            lines.extend([
                "\n" + "-"*70,
                "Ground Truth Structure:"
            ])
            for edge in self.dataset.get_ground_truth_edges():
                lines.append(f"  {edge[0]} â†’ {edge[1]}")

        return "\n".join(lines)
