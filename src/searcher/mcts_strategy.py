# æœªæ¥çš„ MCTS ç­–ç•¥
from abc import ABC, abstractmethod
from typing import Dict, Optional
import os
import json
import copy
import math

from utils.metrics import compute_metrics
from .search_strategy import SearchStrategy



class MCTSNode: # TODO:
    """MCTS æ ‘èŠ‚ç‚¹"""
    
    def __init__(self, graph: Dict, parent: Optional['MCTSNode'] = None, 
                 ll: float = float('-inf'), iteration: int = 0):
        self.graph = graph  # å½“å‰å›¾ç»“æ„
        self.parent = parent  # çˆ¶èŠ‚ç‚¹
        self.children = []  # å­èŠ‚ç‚¹åˆ—è¡¨
        self.visits = 0  # è®¿é—®æ¬¡æ•°
        self.value = 0.0  # ç´¯è®¡å¥–åŠ±
        self.ll = ll  # å½“å‰å›¾çš„log-likelihood
        self.iteration = iteration  # å¯¹åº”çš„è¿­ä»£æ¬¡æ•°
        self.is_fully_expanded = False  # æ˜¯å¦å·²å®Œå…¨æ‰©å±•
    
    def is_leaf(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¶èŠ‚ç‚¹"""
        return len(self.children) == 0
    
    def best_child(self, exploration_weight: float = 1.414) -> 'MCTSNode':
        """ä½¿ç”¨UCB1é€‰æ‹©æœ€ä½³å­èŠ‚ç‚¹"""
        best_score = float('-inf')
        best_child = None
        
        for child in self.children:
            if child.visits == 0:
                # ä¼˜å…ˆé€‰æ‹©æœªè®¿é—®çš„èŠ‚ç‚¹
                return child
            
            # UCB1å…¬å¼: exploitation + exploration
            exploitation = child.value / child.visits
            exploration = exploration_weight * math.sqrt(
                math.log(self.visits) / child.visits
            )
            ucb_score = exploitation + exploration
            
            if ucb_score > best_score:
                best_score = ucb_score
                best_child = child
        
        return best_child
    
    def most_visited_child(self) -> 'MCTSNode':
        """è¿”å›è®¿é—®æ¬¡æ•°æœ€å¤šçš„å­èŠ‚ç‚¹ï¼ˆæœ€ç»ˆé€‰æ‹©ï¼‰"""
        if not self.children:
            return None
        return max(self.children, key=lambda c: c.visits)


class MCTSStrategy(SearchStrategy):
    """è’™ç‰¹å¡æ´›æ ‘æœç´¢ç­–ç•¥"""
    
    def __init__(self, pipeline, num_simulations: int = 100, 
                 exploration_weight: float = 1.414, max_depth: int = 5):
        super().__init__(pipeline)
        self.num_simulations = num_simulations  # æ¯æ¬¡è¿­ä»£çš„æ¨¡æ‹Ÿæ¬¡æ•°
        self.exploration_weight = exploration_weight  # UCB1æ¢ç´¢æƒé‡
        self.max_depth = max_depth  # æœ€å¤§æœç´¢æ·±åº¦
        self.root = None  # æœç´¢æ ‘æ ¹èŠ‚ç‚¹
    
    def search(
        self,
        num_iterations: int = 3,
        early_stopping_patience: int = 3,  # MCTS æš‚ä¸å¼ºåˆ¶ä½¿ç”¨ï¼Œä½†ä¿æŒæ¥å£ä¸€è‡´
        num_epochs: int = 100,
        learning_rate: float = 0.01,
        temperature: float = 0.6,
        llm_model_name: str = "gpt-4o",
        max_tokens: int = 4096,
        verbose: bool = True,
        max_retries: int = 10,
        use_local_amendment: bool = True,
        llm_only: bool = False,
        choose_best: bool = False
    ) -> Dict:
        """MCTS æœç´¢ä¸»å¾ªç¯"""
        
        print(f"\n{'='*70}")
        print(f"SEARCH STRATEGY: Monte Carlo Tree Search (MCTS)")
        if llm_only:
            print(f"MODE: LLM-only (Stop after first successful global graph)")
        print(f"Simulations per iteration: {self.num_simulations}")
        print(f"Exploration weight: {self.exploration_weight}")
        print(f"Max depth: {self.max_depth}")
        print(f"Choose Best Initial: {choose_best}")
        print(f"{'='*70}\n")
        
        best_graph = None
        best_ll = float('-inf')
        
        # ç”Ÿæˆåˆå§‹å›¾
        print(f"ğŸŒ± Generating initial graph...")
        if choose_best and self.pipeline.use_baseline_reference:
            print(f"\n[Choose Best] Comparing Baseline vs Global LLM initial graph...")
            baseline_graph = next(iter(self.pipeline.baseline_structured_graphs.values()))
            global_graph = self._generate_initial_graph(
                llm_model_name, temperature, max_tokens, 
                max_retries, num_epochs, learning_rate, verbose
            )
            
            if global_graph is not None:
                # è¯„ä¼°ä¸¤è€…
                self._evaluate_graph(baseline_graph, num_epochs, learning_rate, verbose=False)
                self._evaluate_graph(global_graph, num_epochs, learning_rate, verbose=False)
                
                baseline_bic = baseline_graph['metadata'].get('bic', float('inf'))
                global_bic = global_graph['metadata'].get('bic', float('inf'))
                
                print(f"  - Baseline BIC: {baseline_bic:.2f}")
                print(f"  - Global LLM BIC: {global_bic:.2f}")
                
                if global_bic < baseline_bic:
                    print(f"  ğŸ† Global LLM wins!")
                    initial_graph = global_graph
                else:
                    print(f"  ğŸ† Baseline wins!")
                    initial_graph = baseline_graph
            else:
                print(f"  âš ï¸ Global LLM generation failed, falling back to Baseline.")
                initial_graph = baseline_graph
        else:
            initial_graph = self._generate_initial_graph(
                llm_model_name, temperature, max_tokens, 
                max_retries, num_epochs, learning_rate, verbose
            )
        
        if initial_graph is None:
            print("âŒ Failed to generate initial graph")
            return {
                'best_graph': None,
                'best_ll': float('-inf'),
                'current_graph': None,
                'history': self.iteration_history
            }
        
        # è¯„ä¼°åˆå§‹å›¾
        initial_ll = self._evaluate_graph(initial_graph, num_epochs, learning_rate, verbose)
        best_graph = copy.deepcopy(initial_graph)
        best_ll = initial_ll
        
        # è®°å½•åˆå§‹å›¾
        from utils.metrics import compute_metrics
        initial_metrics = compute_metrics(self.pipeline, initial_graph)
        initial_graph['metadata']['evaluation_metrics'] = initial_metrics
        
        self.iteration_history.append({
            'iteration': 0,
            'graph': initial_graph,
            'accepted': True,
            'best_ll': best_ll,
            'current_ll': initial_ll,
            'metrics': initial_metrics,
            'results': {
                'log_likelihood': initial_ll,
                'bic': initial_graph['metadata'].get('bic', None),
                'num_edges': initial_graph['metadata']['num_edges']
            }
        })
        
        # self._save_graph(initial_graph, 0) # TODO: currently not used
        
        if self.pipeline.dataset is not None:
            self.pipeline._evaluate_against_ground_truth(initial_graph)
        
        # LLM-only æ¨¡å¼ï¼šè·å¾—ç¬¬ä¸€ä¸ªæˆåŠŸçš„å›¾åç«‹å³åœæ­¢
        if llm_only:
            print(f"\nâœ… [LLM-only] Initial successful graph obtained. Terminating MCTS.")
            return {
                'best_graph': best_graph,
                'best_ll': best_ll,
                'current_graph': best_graph,
                'history': self.iteration_history
            }

        # åˆå§‹åŒ–MCTSæ ¹èŠ‚ç‚¹
        self.root = MCTSNode(initial_graph, parent=None, ll=initial_ll, iteration=0)
        self.root.visits = 1
        self.root.value = initial_ll
        
        print(f"âœ… Initial graph: LL = {initial_ll:.4f}\n")
        
        # MCTSè¿­ä»£
        for t in range(1, num_iterations):
            print(f"\nğŸ”„ MCTS ITERATION {t}/{num_iterations-1}")
            print(f"Current best LL: {best_ll:.4f}")
            
            # æ‰§è¡Œå¤šæ¬¡MCTSæ¨¡æ‹Ÿ
            for sim in range(self.num_simulations):
                if verbose and (sim + 1) % 10 == 0:
                    print(f"  Simulation {sim + 1}/{self.num_simulations}...")
                
                # MCTSå››ä¸ªæ­¥éª¤
                leaf = self._select(self.root)  # é€‰æ‹©
                child = self._expand(leaf, t, llm_model_name, temperature, 
                                    max_tokens, max_retries, num_epochs, 
                                    learning_rate, verbose)  # æ‰©å±•
                
                if child is not None:
                    reward = self._simulate(child)  # æ¨¡æ‹Ÿï¼ˆè¯„ä¼°ï¼‰
                    self._backpropagate(child, reward)  # å›æº¯
            
            # é€‰æ‹©æœ€ä½³å­èŠ‚ç‚¹ä½œä¸ºä¸‹ä¸€ä¸ªæ ¹èŠ‚ç‚¹
            if self.root.children:
                best_child = self.root.most_visited_child()
                
                if best_child and best_child.ll > best_ll:
                    best_ll = best_child.ll
                    best_graph = copy.deepcopy(best_child.graph)
                    print(f"\nâœ… NEW BEST: LL = {best_ll:.4f} (from {best_child.visits} visits)")
                
                # æ›´æ–°æ ¹èŠ‚ç‚¹åˆ°æœ€ä½³å­èŠ‚ç‚¹
                self.root = best_child
                self.root.parent = None
                
                # è®°å½•åˆ°å†å²
                metrics = compute_metrics(self.pipeline, best_child.graph)
                best_child.graph['metadata']['evaluation_metrics'] = metrics
                
                self.iteration_history.append({
                    'iteration': t,
                    'graph': best_child.graph,
                    'accepted': True,
                    'best_ll': best_ll,
                    'current_ll': best_child.ll,
                    'metrics': metrics,
                    'results': {
                        'log_likelihood': best_child.ll,
                        'bic': best_child.graph['metadata'].get('bic', None),
                        'num_edges': best_child.graph['metadata']['num_edges']
                    }
                })
                
                # self._save_graph(best_child.graph, t) # TODO: currently not used
                
                if self.pipeline.dataset is not None:
                    self.pipeline._evaluate_against_ground_truth(best_child.graph)
            else:
                print(f"\nâš ï¸ No valid children found, stopping MCTS")
                break
        
        return {
            'best_graph': best_graph,
            'best_ll': best_ll,
            'current_graph': self.root.graph if self.root else best_graph,
            'history': self.iteration_history
        }
    
    def _select(self, node: MCTSNode) -> MCTSNode:
        """é€‰æ‹©é˜¶æ®µï¼šä»æ ¹èŠ‚ç‚¹é€‰æ‹©åˆ°å¶èŠ‚ç‚¹"""
        current = node
        depth = 0
        
        while not current.is_leaf() and depth < self.max_depth:
            current = current.best_child(self.exploration_weight)
            depth += 1
        
        return current
    
    def _expand(self, node: MCTSNode, iteration: int, llm_model_name: str,
                temperature: float, max_tokens: int, max_retries: int,
                num_epochs: int, learning_rate: float, verbose: bool) -> Optional[MCTSNode]:
        """æ‰©å±•é˜¶æ®µï¼šä¸ºå¶èŠ‚ç‚¹ç”Ÿæˆæ–°çš„å­èŠ‚ç‚¹"""
        
        if node.is_fully_expanded:
            return None
        
        # ===== æ–°å¢ï¼šå¹²é¢„å®éªŒè¯¢é—®é˜¶æ®µ =====
        all_evidence = None
        candidate_operations = None
        if self.pipeline.use_intervention_test:
            current_edge_notes = node.graph['metadata'].get('edge_notes', {}) if node.graph and 'metadata' in node.graph else {}
            experiments, _, candidate_operations = self.pipeline.hypothesis_generator.propose_experiments(
                variable_list=self.pipeline.variable_list,
                domain_name=self.pipeline.domain_name,
                domain_context=self.pipeline.domain_context,
                previous_graph=node.graph,
                num_experiments=self.pipeline.num_intervention_experiments,
                model=llm_model_name,
                temperature=temperature,
                edge_notes=current_edge_notes
            )
            if experiments:
                new_evidence = self.pipeline.intervention_tester.run_experiments(experiments)
                if new_evidence:
                    self.pipeline.accumulated_evidence.append(new_evidence)
                    self.pipeline.policy_verifier.update_evidence(new_evidence)
        
        all_evidence = "\n".join(self.pipeline.accumulated_evidence) if self.pipeline.accumulated_evidence else None

        # ä½¿ç”¨LLMç”Ÿæˆå±€éƒ¨ä¿®æ”¹
        modified_graph, validation_info = self.pipeline.hypothesis_generator.generate_hypothesis(
            variable_list=self.pipeline.variable_list,
            domain_name=self.pipeline.domain_name,
            domain_context=self.pipeline.domain_context,
            previous_graph=node.graph,
            memory=None,
            iteration=iteration,
            model=llm_model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            use_local_amendment=True,
            skeleton_constraints=getattr(self.pipeline, 'skeleton_constraints', None),
            interventional_evidence=all_evidence,
            candidate_operations=candidate_operations
        )

        # Check that graph exists and validation passed
        if modified_graph is None or not (validation_info and validation_info.get('success', True)):
            node.is_fully_expanded = True
            return None
        
        # ===== å¼ºåˆ¶ç­–ç•¥æ ¡éªŒ (EVIDENCE-FIRST POLICY) =====
        proposed_ops = modified_graph['metadata'].get('proposed_operations')
        
        if proposed_ops:
            policy_violations = self.pipeline.policy_verifier.verify_operations(proposed_ops)
            
            if policy_violations:
                print(f"âš ï¸  EVIDENCE-FIRST POLICY VIOLATION DETECTED in MCTS expansion!")
                for violation in policy_violations:
                    print(f"   - {violation}")
                # MCTS ä¸­ï¼Œå¦‚æœè¿åç­–ç•¥ï¼Œæˆ‘ä»¬è®¤ä¸ºè¯¥æ‰©å±•æ— æ•ˆ
                return None
        else:
            # åˆå§‹ç”Ÿæˆæˆ–å…¨å±€ä¿®æ­£ï¼Œè·³è¿‡é’ˆå¯¹æ“ä½œçš„æ ¡éªŒ
            pass
        
        # è¯„ä¼°æ–°å›¾
        ll = self._evaluate_graph(modified_graph, num_epochs, learning_rate, verbose=False)
        
        # åº”ç”¨refinementï¼ˆå¦‚æœå¯ç”¨ï¼‰
        modified_graph, ll = self._apply_refinements(
            modified_graph, ll, iteration, num_epochs, learning_rate, verbose=False
        )
        
        # åˆ›å»ºå­èŠ‚ç‚¹
        child = MCTSNode(modified_graph, parent=node, ll=ll, iteration=iteration)
        node.children.append(child)
        
        return child
    
    def _simulate(self, node: MCTSNode) -> float:
        """æ¨¡æ‹Ÿé˜¶æ®µï¼šè¯„ä¼°èŠ‚ç‚¹çš„ä»·å€¼ï¼ˆç›´æ¥è¿”å›LLï¼‰"""
        # å¯¹äºå› æœå›¾æœç´¢ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨LLä½œä¸ºå¥–åŠ±
        # ä¸éœ€è¦éšæœºrolloutï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†ç¡®å®šçš„è¯„ä¼°æŒ‡æ ‡
        return node.ll
    
    def _backpropagate(self, node: MCTSNode, reward: float):
        """å›æº¯é˜¶æ®µï¼šæ›´æ–°è·¯å¾„ä¸Šæ‰€æœ‰èŠ‚ç‚¹çš„ç»Ÿè®¡ä¿¡æ¯"""
        current = node
        
        while current is not None:
            current.visits += 1
            current.value += reward
            current = current.parent
    
    def _generate_initial_graph(self, llm_model_name: str, temperature: float,
                                max_tokens: int, max_retries: int, num_epochs: int,
                                learning_rate: float, verbose: bool) -> Optional[Dict]:
        """ç”Ÿæˆåˆå§‹å›¾ï¼ˆå¸¦é‡è¯•ï¼‰"""
        
        failed_attempts = []
        
        # åˆå§‹å¹²é¢„æµ‹è¯•
        all_evidence = None
        candidate_operations = None
        if self.pipeline.use_intervention_test:
            experiments, _, candidate_operations = self.pipeline.hypothesis_generator.propose_experiments(
                variable_list=self.pipeline.variable_list,
                domain_name=self.pipeline.domain_name,
                domain_context=self.pipeline.domain_context,
                previous_graph=None,
                num_experiments=self.pipeline.num_intervention_experiments,
                model=llm_model_name,
                temperature=temperature,
                edge_notes={}
            )
            if experiments:
                new_evidence = self.pipeline.intervention_tester.run_experiments(experiments)
                if new_evidence:
                    self.pipeline.accumulated_evidence.append(new_evidence)
                    self.pipeline.policy_verifier.update_evidence(new_evidence)
        
        all_evidence = "\n".join(self.pipeline.accumulated_evidence) if self.pipeline.accumulated_evidence else None
        
        for retry in range(max_retries):
            try:
                structured_graph, validation_info = self.pipeline.hypothesis_generator.generate_hypothesis(
                    variable_list=self.pipeline.variable_list,
                    domain_name=self.pipeline.domain_name,
                    domain_context=self.pipeline.domain_context,
                    previous_graph=None,
                    memory=None,
                    iteration=0,
                    model=llm_model_name,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    use_local_amendment=False,
                    skeleton_constraints=getattr(self.pipeline, 'skeleton_constraints', None),
                    failed_attempts=failed_attempts if failed_attempts else None,
                    baseline_reference=getattr(self.pipeline, 'baseline_reference_text', None),
                    interventional_evidence=all_evidence,
                    candidate_operations=candidate_operations
                )
                
                # Check both that graph exists and validation passed
                if structured_graph is not None and validation_info and validation_info.get('success', True):
                    return structured_graph
                else:
                    error_messages = validation_info.get('error_messages', []) if validation_info else []
                    error_msg = '; '.join(error_messages) if error_messages else ('Graph is None' if structured_graph is None else 'Unknown error')
                    print(f"  [Retry {retry+1}] Validation failed: {error_msg}")
                    
                    if structured_graph:
                        failed_attempts.append({
                            'graph': structured_graph,
                            'error': error_msg,
                            'cycle_path': validation_info.get('cycle_path')
                        })
                        
            except (TypeError, AttributeError) as e:
                # å‚æ•°é”™è¯¯æˆ–å±æ€§é”™è¯¯ï¼Œç«‹å³æŠ›å‡ºï¼Œä¸é‡è¯•
                print(f"\nâŒ FATAL ERROR: {type(e).__name__}: {str(e)}")
                print("This is a programming error, not a retry-able failure.")
                raise
            except Exception as e:
                # å…¶ä»–é”™è¯¯ï¼Œå¯ä»¥é‡è¯•
                print(f"  [Retry {retry+1}] Exception: {str(e)}")
                if retry == max_retries - 1:
                    # æœ€åä¸€æ¬¡é‡è¯•ä¹Ÿå¤±è´¥äº†ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    print(f"\nâŒ All {max_retries} retries failed.")
                    raise
        
        return None
    
    def _evaluate_graph(self, graph: Dict, num_epochs: int, 
                       learning_rate: float, verbose: bool = False) -> float:
        """è¯„ä¼°å›¾çš„log-likelihood"""
        from model_fitting import ModelFittingEngine
        
        engine = ModelFittingEngine(device=self.pipeline.device)
        
        results = engine.fit(
            structured_graph=graph,
            data=self.pipeline.data,
            interventions=self.pipeline.interventions,
            variable_type=self.pipeline.variable_type,  # ä¼ é€’å˜é‡ç±»å‹
            num_epochs=num_epochs,
            learning_rate=learning_rate,
            verbose=verbose
        )
        
        return results['log_likelihood']
    
    def _apply_refinements(self, graph: Dict, ll: float, iteration: int,
                          num_epochs: int, learning_rate: float, 
                          verbose: bool) -> tuple:
        """åº”ç”¨NOTEARSå’ŒGreedy refinementï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        
        # NOTEARS refinement
        if (self.pipeline.use_notears_refinement and 
            iteration >= self.pipeline.notears_start_iter):
            
            if verbose:
                print(f"\nğŸ”§ Applying NOTEARS refinement...")
            
            refined_graph = self.pipeline.notears_refiner.refine_graph(
                initial_graph=graph,
                data=self.pipeline.data,
                num_epochs=num_epochs,
                lr=learning_rate
            )
            
            if refined_graph is not None:
                refined_ll = self._evaluate_graph(
                    refined_graph, num_epochs, learning_rate, verbose=False
                )
                
                if refined_ll > ll:
                    if verbose:
                        print(f"  âœ… NOTEARS improved LL: {ll:.4f} â†’ {refined_ll:.4f}")
                    graph = refined_graph
                    ll = refined_ll
                else:
                    if verbose:
                        print(f"  âš ï¸ NOTEARS did not improve (kept original)")
        
        # Greedy refinement
        if (self.pipeline.use_greedy_refinement and 
            iteration >= self.pipeline.greedy_start_iter):
            
            if verbose:
                print(f"\nğŸ”§ Applying Greedy refinement...")
            
            refined_graph = self.pipeline.greedy_refiner.refine_graph(
                initial_graph=graph
            )
            
            if refined_graph is not None:
                refined_ll = self._evaluate_graph(
                    refined_graph, num_epochs, learning_rate, verbose=False
                )
                
                if refined_ll > ll:
                    if verbose:
                        print(f"  âœ… Greedy improved LL: {ll:.4f} â†’ {refined_ll:.4f}")
                    graph = refined_graph
                    ll = refined_ll
                else:
                    if verbose:
                        print(f"  âš ï¸ Greedy did not improve (kept original)")
        
        return graph, ll
    
    def _save_graph(self, graph: Dict, iteration: int):
        """ä¿å­˜å›¾åˆ°JSONæ–‡ä»¶"""
        graph_path = os.path.join(self.pipeline.output_dir, f"graph_t{iteration}.json")
        with open(graph_path, 'w') as f:
            json.dump(graph, f, indent=2)
